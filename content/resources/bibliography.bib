
@article{zhai_effects_2024,
	title = {The effects of over-reliance on {AI} dialogue systems on students' cognitive abilities: a systematic review},
	volume = {11},
	issn = {2196-7091},
	shorttitle = {The effects of over-reliance on {AI} dialogue systems on students' cognitive abilities},
	url = {https://doi.org/10.1186/s40561-024-00316-7},
	doi = {10.1186/s40561-024-00316-7},
	abstract = {The growing integration of artificial intelligence (AI) dialogue systems within educational and research settings highlights the importance of learning aids. Despite examination of the ethical concerns associated with these technologies, there is a noticeable gap in investigations on how these ethical issues of AI contribute to students’ over-reliance on AI dialogue systems, and how such over-reliance affects students’ cognitive abilities. Overreliance on AI occurs when users accept AI-generated recommendations without question, leading to errors in task performance in the context of decision-making. This typically arises when individuals struggle to assess the reliability of AI or how much trust to place in its suggestions. This systematic review investigates how students’ over-reliance on AI dialogue systems, particularly those embedded with generative models for academic research and learning, affects their critical cognitive capabilities including decision-making, critical thinking, and analytical reasoning. By using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our systematic review evaluated a body of literature addressing the contributing factors and effects of such over-reliance within educational and research contexts. The comprehensive literature review spanned 14 articles retrieved from four distinguished databases: ProQuest, IEEE Xplore, ScienceDirect, and Web of Science. Our findings indicate that over-reliance stemming from ethical issues of AI impacts cognitive abilities, as individuals increasingly favor fast and optimal solutions over slow ones constrained by practicality. This tendency explains why users prefer efficient cognitive shortcuts, or heuristics, even amidst the ethical issues presented by AI technologies.},
	number = {1},
	urldate = {2025-11-04},
	journal = {Smart Learning Environments},
	author = {Zhai, Chunpeng and Wibowo, Santoso and Li, Lily D.},
	month = jun,
	year = {2024},
	keywords = {Analytical thinking, Cognitive abilities, Critical thinking, Decision-making, Ethical issues of AI, Generative AI},
	pages = {28},
}

@misc{noauthor_evaluating_nodate,
	title = {Evaluating {Human}-{AI} {Collaboration}: {A} {Review} and {Methodological} {Framework}},
	url = {https://arxiv.org/html/2407.19098v1},
	urldate = {2025-10-21},
}

@article{cao_understanding_2021,
	title = {Understanding managers’ attitudes and behavioral intentions towards using artificial intelligence for organizational decision-making},
	volume = {106},
	issn = {0166-4972},
	url = {https://www.sciencedirect.com/science/article/pii/S0166497221000936},
	doi = {10.1016/j.technovation.2021.102312},
	abstract = {While using artificial intelligence (AI) could improve organizational decision-making, it also creates challenges associated with the “dark side” of AI. However, there is a lack of research on managers' attitudes and intentions to use AI for decision making. To address this gap, we develop an integrated AI acceptance-avoidance model (IAAAM) to consider both the positive and negative factors that collectively influence managers' attitudes and behavioral intentions towards using AI. The research model is tested through a large-scale questionnaire survey of 269 UK business managers. Our findings suggest that IAAAM provides a more comprehensive model for explaining and predicting managers' attitudes and behavioral intentions towards using AI. Our research contributes conceptually and empirically to the emerging literature on using AI for organizational decision-making. Further, regarding the practical implications of using AI for organizational decision-making, we highlight the importance of developing favorable facilitating conditions, having an effective mechanism to alleviate managers’ personal concerns, and having a balanced consideration of both the benefits and the dark side associated with using AI.},
	urldate = {2025-10-21},
	journal = {Technovation},
	author = {Cao, Guangming and Duan, Yanqing and Edwards, John S. and Dwivedi, Yogesh K.},
	month = aug,
	year = {2021},
	keywords = {AI adoption, Artificial intelligence, Integrated AI acceptance-Avoidance model (IAAAM), Organizational decision-making, Technology threat avoidance theory (TTAT), Unified theory of acceptance and use of technology (UTAUT)},
	pages = {102312},
}

@article{gillath_attachment_2021,
	title = {Attachment and trust in artificial intelligence},
	volume = {115},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S074756322030354X},
	doi = {10.1016/j.chb.2020.106607},
	abstract = {Lack of trust is one of the main obstacles standing in the way of taking full advantage of the benefits artificial intelligence (AI) has to offer. Most research on trust in AI focuses on cognitive ways to boost trust. Here, instead, we focus on boosting trust in AI via affective means. Specifically, we tested and found associations between one's attachment style—an individual difference representing the way people feel, think, and behave in relationships—and trust in AI. In Study 1 we found that attachment anxiety predicted less trust. In Study 2, we found that enhancing attachment anxiety reduced trust, whereas enhancing attachment security increased trust in AI. In Study 3, we found that exposure to attachment security cues (but not positive affect cues) resulted in increased trust as compared with exposure to neutral cues. Overall, our findings demonstrate an association between attachment security and trust in AI, and support the ability to increase trust in AI via attachment security priming.},
	urldate = {2025-10-21},
	journal = {Computers in Human Behavior},
	author = {Gillath, Omri and Ai, Ting and Branicky, Michael S. and Keshmiri, Shawn and Davison, Robert B. and Spaulding, Ryan},
	month = feb,
	year = {2021},
	keywords = {Artificial intelligence, Attachment style, Close relationships, Trust},
	pages = {106607},
}

@article{wen_trust_2025,
	title = {Trust and {AI} weight: human-{AI} collaboration in organizational management decision-making},
	volume = {3},
	issn = {2813-771X},
	shorttitle = {Trust and {AI} weight},
	url = {https://www.frontiersin.org/journals/organizational-psychology/articles/10.3389/forgp.2025.1419403/full},
	doi = {10.3389/forgp.2025.1419403},
	abstract = {IntroductionThe emergence of Artificial Intelligence (AI) has revolutionized decision-making in human resource management. Since human and AI each possesses distinct strengths in the realm of decision-making, the synergy between human and AI agent has the potential to significantly enhance both the efficiency and the quality of managerial decision-making processes. Although assigning decision weights to AI agents presents innovative avenues for human-AI collaboration, the underlying mechanisms driving the allocation of decision weights to AI agents remain inadequately understood. To elucidate these mechanisms, this paper examines the influence of trust in AI on AI weight allocation within the framework of human-AI cooperation, leveraging the Socio-Cognitive Model of Trust (SCMT).MethodsWe conducted a series of survey studies involving scenario-based decision-making tasks. Study 1 examined the relationship between trust in AI and AI weight among 111 managers about employee recruitment tasks. Study 2 surveyed 210 managers using employee performance evaluation tasks.ResultsThe results of Study 1 indicated that trust in AI enhances the decisional weight attributed to AI agents, and willingness to collaborate with AI mediates trust in AI and the weight of AI in personnel selection. The findings of Study 2 revealed that the perceived free will of AI agents negatively moderates the relationship between trust in AI and willing to collaborate with AI, such that the relationship is weaker when individuals perceive a higher degree of free will in AI agents than a lower degree.DiscussionTheoretically, this paper advances the understanding of the function of trust in human-AI interaction by exploring the trust development from attitude to act in human-AI cooperative decision-making. Practically, it offers valuable insights into the design of AI agent and organizational management within the context of human-AI collaboration.},
	language = {English},
	urldate = {2025-10-21},
	journal = {Frontiers in Organizational Psychology},
	author = {Wen, Yanjun and Wang, Jiale and Chen, Xiaoxi},
	month = jun,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {AI weight, decision-making, human-AI cooperation, socio-cognitive model of trust, trust},
}

@article{chen_systematic_2025,
	title = {A {Systematic} {Review} of {User} {Attitudes} {Toward} {GenAI}: {Influencing} {Factors} and {Industry} {Perspectives}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-3200},
	shorttitle = {A {Systematic} {Review} of {User} {Attitudes} {Toward} {GenAI}},
	url = {https://www.mdpi.com/2079-3200/13/7/78},
	doi = {10.3390/jintelligence13070078},
	abstract = {In the era of GenAI, user attitude—shaped by cognition, emotion, and behavior—plays a critical role in the sustainable development of human–AI interaction. Human creativity and intelligence, as core drivers of social progress, are important factors influencing user attitudes. This paper systematically reviews 243 peer-reviewed studies on GenAI user attitudes published since 2019, identifying major research methods and theoretical perspectives, including the Technology Acceptance Model (TAM), the Unified Theory of Acceptance and Use of Technology (UTAUT), and the AI Device Use Acceptance (AIDUA) model. Drawing on contemporary creativity theories—such as Sternberg’s Theory of Successful Intelligence, the 4C Model by Kaufman and Beghetto, and the Dynamic Creativity Framework—we analyze how creativity and intelligence are conceptualized in current studies and how they affect user responses to GenAI. Through cross-cultural analysis and multimodal comparison, this review offers a comprehensive understanding of the interplay between GenAI and human creativity, aiming to support more inclusive and sustainable human–AI collaboration.},
	language = {en},
	number = {7},
	urldate = {2025-10-21},
	journal = {Journal of Intelligence},
	author = {Chen, Junjie and Xie, Wei and Xie, Qing and Hu, Anshu and Qiao, Yiran and Wan, Ruoyu and Liu, Yuhan},
	month = jul,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GenAI, acceptance, application domains, bibliometric analysis, creativity, intelligence, user attitudes},
	pages = {78},
}

@incollection{tversky_framing_1986,
	series = {Logic, {Methodology} and {Philosophy} of {Science} {VII}},
	title = {The {Framing} of {Decisions} and the {Evaluation} of {Prospects}},
	volume = {114},
	url = {https://www.sciencedirect.com/science/article/pii/S0049237X09707104},
	abstract = {The chapter presents a series of demonstrations in which seemingly inconsequential changes in the formulation of choice problems caused significant shifts of preference. The inconsistencies were traced to the interaction of two sets of factors: variation in the framing of acts, contingencies and outcomes, and the characteristic non-linearities of values and decision weights. The demonstrated effects are large and systematic, although by no means universal. They occur when the outcomes concern the loss of human lives as well as in choices about money; they are not restricted to hypothetical questions and are not eliminated by monetary incentives. The chapter is concerned primarily with the descriptive question of how decisions are made and emphasizes that the psychology of choice is also relevant to the normative question of how decisions ought to be made. The framing of acts and outcomes can also reflect the acceptance or rejection of responsibility for particular consequences, and the deliberate manipulation of framing is commonly used as an instrument of self-control. When framing influences the experience of consequences, the adoption of a decision frame is an ethically significant act.},
	language = {en},
	urldate = {2025-09-29},
	booktitle = {Studies in {Logic} and the {Foundations} of {Mathematics}},
	publisher = {Elsevier},
	author = {Tversky, Amos and Kahneman, Daniel},
	editor = {Barcan Marcus, Ruth and Dorn, Georg J. W. and Weingartner, Paul},
	month = jan,
	year = {1986},
	doi = {10.1016/S0049-237X(09)70710-4},
	pages = {503--520},
}

@article{freling_when_2014,
	title = {When not to accentuate the positive: {Re}-examining valence effects in attribute framing},
	volume = {124},
	issn = {0749-5978},
	shorttitle = {When \textit{not} to accentuate the positive},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597814000090},
	doi = {10.1016/j.obhdp.2013.12.007},
	abstract = {While the expanding body of attribute framing literature provides keen insights into individual judgments and evaluations, a lack of theoretical perspective inhibits scholars from more fully extending research foci beyond a relatively straightforward examination of message content. The current research applies construal level theory to attribute framing research. The authors conduct a meta-analysis of 107 published articles and then conceptually expand this knowledge base by synthesizing attribute framing research and construal level concepts. Results suggest that attribute framing is most effective when there is congruence between the construal level evoked in a frame and the evaluator’s psychological distance from the framed event. A follow-up experiment confirms that the congruence between a frame’s construal level and psychological distance—not simply its valence—appears to be driving attribute framing effects. This research proposes to shift the focus in attribute framing research from that of message composition to a more complex relationship between the message and the recipient.},
	number = {2},
	urldate = {2025-09-28},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Freling, Traci H. and Vincent, Leslie H. and Henard, David H.},
	month = jul,
	year = {2014},
	keywords = {Construal level theory, Framing effects, Message framing, Meta-analysis},
	pages = {95--109},
}

@article{cugurullo_fear_2024,
	title = {Fear of {AI}: an inquiry into the adoption of autonomous cars in spite of fear, and a theoretical framework for the study of artificial intelligence technology acceptance},
	volume = {39},
	issn = {1435-5655},
	shorttitle = {Fear of {AI}},
	url = {https://doi.org/10.1007/s00146-022-01598-6},
	doi = {10.1007/s00146-022-01598-6},
	abstract = {Artificial intelligence (AI) is becoming part of the everyday. During this transition, people’s intention to use AI technologies is still unclear and emotions such as fear are influencing it. In this paper, we focus on autonomous cars to first verify empirically the extent to which people fear AI and then examine the impact that fear has on their intention to use AI-driven vehicles. Our research is based on a systematic survey and it reveals that while individuals are largely afraid of cars that are driven by AI, they are nonetheless willing to adopt this technology as soon as possible. To explain this tension, we extend our analysis beyond just fear and show that people also believe that AI-driven cars will generate many individual, urban and global benefits. Subsequently, we employ our empirical findings as the foundations of a theoretical framework meant to illustrate the main factors that people ponder when they consider the use of AI tech. In addition to offering a comprehensive theoretical framework for the study of AI technology acceptance, this paper provides a nuanced understanding of the tension that exists between the fear and adoption of AI, capturing what exactly people fear and intend to do.},
	language = {en},
	number = {4},
	urldate = {2025-09-28},
	journal = {AI \& SOCIETY},
	author = {Cugurullo, Federico and Acheampong, Ransford A.},
	month = aug,
	year = {2024},
	keywords = {Artificial intelligence, Autonomous cars, Fear, Technology acceptance, Theoretical framework, Urban artificial intelligences},
	pages = {1569--1584},
}

@misc{cunningham_how_2025,
	address = {Cambridge, MA 02138},
	title = {How {People} {Use} {ChatGPT}},
	copyright = {National Bureau of Economic Research, 1050 Massachusetts Avenue, Cambridge, MA 02138},
	shorttitle = {How {People} {Use} {ChatGPT}},
	url = {https://www.nber.org/papers/w34255},
	doi = {10.3386/w34255},
	language = {en},
	urldate = {2025-09-28},
	author = {Cunningham, Thomas and Deming, J. David and Hitzig, Zoe and Ong, Christopher and Yan Shan, Carl and Wadman, Kevin},
	month = sep,
	year = {2025},
}

@article{baroni_ai-tam_2022,
	title = {{AI}-{TAM}: a model to investigate user acceptance and collaborative intention in human-in-the-loop {AI} applications},
	volume = {9},
	copyright = {Copyright (c) 2022 Ilaria Baroni, Gloria Re Calegari, Damiano Scandolari, Irene Celino},
	issn = {2330-8001},
	shorttitle = {{AI}-{TAM}},
	url = {https://hcjournal.org/index.php/jhc/article/view/134},
	doi = {10.15346/hc.v9i1.134},
	abstract = {More and more frequently, digital applications make use of Artificial Intelligence (AI) capabilities to provide advanced features; on the other hand, human-in-the-loop approaches are on the rise to involve people in AI-powered pipelines for data collection, results validation and decision-making.
Does the introduction of AI features affect user acceptance? Does the AI result quality affect people willingness to use such applications? Does the additional user effort required in human-in-the-loop mechanisms change the application adoption and use?
This study aims to provide a reference approach to answer those questions. We propose a model that extends the Technology Acceptance Model (TAM) with further constructs explicitly related to AI (user trust in AI and perceived quality of AI output, from XAI literature) and collaborative intention (willingness to contribute to AI pipelines).
We tested the proposed model with an application for car damage claim reporting with AI-powered damage estimation for insurance customers. The results showed that the XAI related factors have a strong and positive effect on the behavioural intention, the perceived usefulness and the ease of use of the application. Moreover, there is a strong link between the behavioural intention and the collaborative intention, indicating that indeed human-in-the-loop approaches can be successfully adopted in final user applications.},
	language = {en},
	number = {1},
	urldate = {2025-09-28},
	journal = {Human Computation},
	author = {Baroni, Ilaria and Calegari, Gloria Re and Scandolari, Damiano and Celino, Irene},
	month = may,
	year = {2022},
	pages = {1--21},
}

@article{ray_can_2019,
	title = {Can {You} {Explain} {That}? {Lucid} {Explanations} {Help} {Human}-{AI} {Collaborative} {Image} {Retrieval}},
	volume = {7},
	shorttitle = {Can {You} {Explain} {That}?},
	doi = {10.1609/hcomp.v7i1.5275},
	abstract = {While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as “helpful” significantly improve human performance, “incorrect” and “unhelpful” explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanation on a human-AI collaborative task.},
	journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	author = {Ray, Arijit and Yao, Yi and Kumar, Rakesh and Divakaran, Ajay and Burachas, Giedrius},
	month = oct,
	year = {2019},
	pages = {153--161},
}

@article{lee_trust_2004,
	title = {Trust in {Automation}: {Designing} for {Appropriate} {Reliance}},
	volume = {46},
	issn = {0018-7208},
	shorttitle = {Trust in {Automation}},
	url = {https://journals.sagepub.com/action/showAbstract},
	doi = {10.1518/hfes.46.1.50_30392},
	abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
	language = {EN},
	number = {1},
	urldate = {2025-09-23},
	journal = {Human Factors},
	author = {Lee, John D. and See, Katrina A.},
	month = mar,
	year = {2004},
	note = {Publisher: SAGE Publications Inc},
	pages = {50--80},
}

@incollection{sasmannshausen_vertrauen_2020,
	address = {Wiesbaden},
	title = {Vertrauen in {KI} – {Eine} empirische {Analyse} innerhalb des {Produktionsmanagements}},
	isbn = {978-3-658-29550-9},
	url = {https://doi.org/10.1007/978-3-658-29550-9_10},
	abstract = {Ob Industrie 4.0, Big Data, Predictive Analytics oder Robotik – die digitale Transformation hat viele Facetten. Sie führt aber nicht nur zu einem Paradigmenwechsel in der industriellen Produktion. Auch komplexe kognitive Tätigkeiten sind durch die fortschreitende Entwicklung der künstlichen Intelligenz (KI) einem Wandel unterzogen. Smarte Assistenten halten Einzug in die Arbeitswelt und erfordern eine Kooperation von KI und Mensch. KI agiert anders als bisherige Systeme – autonom statt automatisch – und somit bisweilen für den Menschen unerwartet, überraschend und nicht immer nachvollziehbar. In dieser Konstellation ist Vertrauen ein essenzieller Faktor, der über das Funktionieren der Mensch-KI-Kooperation entscheidet. Im Rahmen des vorliegenden Beitrages sollen daher mit einer empirischen Analyse innerhalb des Produktionsmanagements die Einflussfaktoren auf das Vertrauen sowie deren Wirkmechanismen identifiziert werden.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Künstliche {Intelligenz} in {Wirtschaft} \& {Gesellschaft}: {Auswirkungen}, {Herausforderungen} \& {Handlungsempfehlungen}},
	publisher = {Springer Fachmedien},
	author = {Saßmannshausen, Till Moritz and Heupel, Thomas},
	editor = {Buchkremer, Rüdiger and Heupel, Thomas and Koch, Oliver},
	year = {2020},
	doi = {10.1007/978-3-658-29550-9_10},
	pages = {169--192},
}

@misc{singh_generative_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Generative {AI} through the {Lens} of {Technology} {Acceptance} {Model}},
	url = {https://papers.ssrn.com/abstract=4953174},
	doi = {10.2139/ssrn.4953174},
	abstract = {This paper applies the Technology Acceptance Model (TAM) to explore the adoption of Generative AI systems like ChatGPT, Claude, Gemini, and Copilot. It analyzes how Perceived Usefulness (PU) and Perceived Ease of Use (PEOU) influence the increasing adoption of these technologies. The study finds that simplifying technical complexity enhances users' perceptions of utility and ease, boosting adoption intentions. Key factors include effective organizational training, intuitive design, and strategic partnerships. The paper also addresses challenges such as trust and ethical concerns, offering insights into user acceptance in the rapidly evolving AI landscape.},
	language = {en},
	urldate = {2025-09-23},
	publisher = {Social Science Research Network},
	author = {Singh, Dr Preet Deep},
	month = sep,
	year = {2024},
	keywords = {Dr Preet Deep Singh, Generative AI through the Lens of Technology Acceptance Model, SSRN},
}

@article{kelly_what_2023,
	title = {What factors contribute to the acceptance of artificial intelligence? {A} systematic review},
	volume = {77},
	issn = {0736-5853},
	shorttitle = {What factors contribute to the acceptance of artificial intelligence?},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585322001587},
	doi = {10.1016/j.tele.2022.101925},
	abstract = {Artificial Intelligence (AI) agents are predicted to infiltrate most industries within the next decade, creating a personal, industrial, and social shift towards the new technology. As a result, there has been a surge of interest and research towards user acceptance of AI technology in recent years. However, the existing research appears dispersed and lacks systematic synthesis, limiting our understanding of user acceptance of AI technologies. To address this gap in the literature, we conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and meta-Analysis guidelines using five databases: EBSCO host, Embase, Inspec (Engineering Village host), Scopus, and Web of Science. Papers were required to focus on both user acceptance and AI technology. Acceptance was defined as the behavioural intention or willingness to use, buy, or try a good or service. A total of 7912 articles were identified in the database search. Sixty articles were included in the review. Most studies (n = 31) did not define AI in their papers, and 38 studies did not define AI for their participants. The extended Technology Acceptance Model (TAM) was the most frequently used theory to assess user acceptance of AI technologies. Perceived usefulness, performance expectancy, attitudes, trust, and effort expectancy significantly and positively predicted behavioural intention, willingness, and use behaviour of AI across multiple industries. However, in some cultural scenarios, it appears that the need for human contact cannot be replicated or replaced by AI, no matter the perceived usefulness or perceived ease of use. Given that most of the methodological approaches present in the literature have relied on self-reported data, further research using naturalistic methods is needed to validate the theoretical model/s that best predict the adoption of AI technologies.},
	urldate = {2025-09-23},
	journal = {Telematics and Informatics},
	author = {Kelly, Sage and Kaye, Sherrie-Anne and Oviedo-Trespalacios, Oscar},
	month = feb,
	year = {2023},
	keywords = {AI, Human factors, Machine learning, Psychosocial models, Social robotics, User acceptance},
	pages = {101925},
}

@article{budnik_can_2025,
	title = {Can {We} {Trust} {Artificial} {Intelligence}?},
	volume = {38},
	doi = {10.1007/s13347-024-00820-1},
	abstract = {In view of the dramatic advancements in the development of artificial intelligence technology in recent years, it has become a commonplace to demand that AI systems be trustworthy. This view presupposes that it is possible to trust AI technology in the first place. The aim of this paper is to challenge this view. In order to do that, it is argued that the philosophy of trust really revolves around the problem of how to square the epistemic and the normative dimensions of trust. Given this double nature of trust it is possible to extract a threefold challenge to the defenders of the possibility of AI trust without presupposing any particular trust theory. They have to show (1) how trust in AI systems is more than mere reliance; (2) how AI systems can become objects of normative expectations; and (3) how the resulting attitude gives human agents reassurance in their interactions with AI systems. In order to demonstrate how difficult this task is, the threefold challenge is then applied to two recent accounts that defend the possibility of trust in AI systems. By way of conclusion it is suggested that instead of trusting AI systems, we should strive to make them reliable.},
	journal = {Philosophy \& Technology},
	author = {Budnik, Christian},
	month = jan,
	year = {2025},
}

@article{kim_communicating_2022,
	title = {Communicating the {Limitations} of {AI}: {The} {Effect} of {Message} {Framing} and {Ownership} on {Trust} in {Artificial} {Intelligence}},
	volume = {39},
	shorttitle = {Communicating the {Limitations} of {AI}},
	doi = {10.1080/10447318.2022.2049134},
	abstract = {Trust plays an essential role in the interaction between humans and artificial intelligence (AI). To promote trust in AI, information about the AI’s performance should be communicated well to the users. Accordingly, this paper investigates how information about AI performance should be presented, focusing on message framing and the ownership of decisions. A 2 (ownership: no ownership vs. ownership) × 3 (message framing: no information vs. negative information vs. positive information) between-subjects experiment was conducted (N = 120). Participants were asked to choose items to help them survive in the desert, supported by an AI decision. The results showed that participants without decision ownership perceived higher trust than those with decision ownership. Also, trust was perceived to be higher when participants were not given performance information than when they were. The results indicate the importance of carefully communicating with AI. The implications of this study are discussed.},
	journal = {International Journal of Human-Computer Interaction},
	author = {Kim, Taenyun and Song, Hayeon},
	month = apr,
	year = {2022},
	pages = {1--11},
}

@techreport{schreiber_psychologie_2020,
	title = {Psychologie und künstliche {Intelligenz} ({KI}) : {Parallelen}, {Chancen}, {Herausforderungen} und ein {Blick} in die nahe {Zukunft}},
	shorttitle = {Psychologie und künstliche {Intelligenz} ({KI})},
	url = {https://digitalcollection.zhaw.ch/handle/11475/19724},
	abstract = {Im vorliegenden Kapitel wird die KI im Kontext der Psychologie, der Lehre des menschlichen Verhaltens und Erlebens, betrachtet. Dabei wird mit der Theorie der Persönlichkeits-System-Interaktionen (PSI-Theorie) (Kuhl 2001) eine psychologische Theorie als Basis genommen, die von zwei Arten der Intelligenz ausgeht, nämlich einer analytischen und einer intuitiven. Die PSI-Theorie eignet sich als Grundlage für eine Reflexion der Chancen und Herausforderungen im Zusammenhang mit der KI, weil sich dabei die konkrete Frage stellt, ob KI „nur“ die analytische Intelligenz abdecken oder ob sie auch intuitiv intelligent sein und dadurch menschliche Züge annehmen kann. Nach einer Auslegeordnung auf der Basis der PSI-Theorie wird ein spezieller Fokus auf die psychologische Intelligenzforschung gelegt. Dabei wird aufgezeigt, dass sich Theorie und Praxis aufgrund der unterschiedlichen erkenntnistheoretischen Schwerpunkte in den vergangenen Jahren auseinanderentwickelt haben und eine neue Praxis- und Forschungsagenda postuliert. Mit Bezug zur Intelligenzforschung werden sodann Gedanken zur Entwicklung der KI in der nahen Zukunft formuliert. Das Kapitel wird abgerundet durch die Beschreibung eines konkreten praxisbezogenen Projektes mit starkem Explorationscharakter, welches zum Ziel hat, KI und psychologische Beratungspraxis in den Bereichen Emotions- und Spracherkennung zu verbinden.},
	language = {de},
	urldate = {2025-09-23},
	institution = {Springer},
	author = {Schreiber, Marc and Gloor, Peter A.},
	month = feb,
	year = {2020},
	doi = {10.1007/978-3-662-60465-6_12},
	note = {ISBN: 9783662604649
Publication Title: Angewandte Psychologie in der Arbeitswelt},
	pages = {161--180},
}

@article{karg_ai_2024,
	title = {In {AI} {We} {Trust}. {Aspekte} des {Vertrauens} in {ChatGPT}},
	url = {https://irf.fhnw.ch/handle/11654/47513},
	abstract = {Die Relevanz von Künstlicher Intelligenz (KI) nimmt zu, ebenso wie die diesbezüglichen Sicherheitsbedenken. Dies führt zur Forderung nach einer vertrauenswürdigeren KI, welcher sich Forschende zunehmend annehmen. Es existiert jedoch keine allgemeine Theorie zu Vertrauen in KI; so dient häufig das Modell Trust in Automation von Lee und See (2004) als Forschungsgrundlage, so kann Automation als Basis von KI betrachtet werden. Gleichzeitig wird jedoch in Frage gestellt, ob dieses Modell auf KI übertragen werden kann. Entsprechend wird in dieser Arbeit im Kontext von ChatGPT untersucht, welche Faktoren das Vertrauen in KI beeinflussen und wie sich dies auf die Nutzungsintention auswirkt. Basierend auf Trust in Automation, ergänzt durch die Variable Nutzungsintention, wurde hierzu ein Pfadmodell abgeleitet. Zudem wurden Einflussfaktoren der Neigung zum Vertrauen sowie die zeitliche Veränderung des Zusammenhangs des Vertrauens im Kontext einer Intervention untersucht. Die Datenerhebung erfolgte anhand validierter Fragebögen. Im Rahmen der Datenanalyse wurden die Daten von insgesamt 105 Studierenden berücksichtigt, während für die ergänzende Längsschnittanalyse 10 Datensätze herangezogen wurden. Die Resultate bestätigen das konzeptuelle Pfadmodell nicht, was eine Respezifikation erforderlich machte. Dies hat dazu geführt, dass zusätzliche Pfade von der Neigung zum Vertrauen zu den drei Faktoren der Vertrauenswürdigkeit identifiziert wurden. Das Modell zeigt mit einer Ausnahme die erwarteten positiven Einflüsse. Dabei ist der Einfluss des Vertrauens auf die Nutzungsintention geringer als erwartet. Die Längsschnittanalyse hat keinerlei Veränderungen zwischen den beiden Messzeitpunkten offenbart. Die Ergebnisse unterstützen grösstenteils die Erwartungen, stellen jedoch auch bisherige Annahmen in Frage. So wird Vertrauen möglicherweise eine grössere Rolle zugeschrieben als tatsächlich vorliegt.},
	language = {de},
	urldate = {2025-09-23},
	author = {Karg, Jona},
	month = sep,
	year = {2024},
	note = {Publisher: Hochschule für Angewandte Psychologie FHNW},
}

@article{lelli_co-intelligence_2025,
	title = {Co-intelligence in design: the importance of trust in artificial intelligence},
	volume = {5},
	shorttitle = {Co-intelligence in design},
	doi = {10.1017/pds.2025.10111},
	abstract = {As Generative Artificial Intelligence (GenAI) gets integrated in design processes, building trust in these systems is critical for effective human-AI collaboration. This study introduces a framework aimed at translating the abstract concept of trust into practical strategies for design teams, focusing on four trust factors: transparency, accountability, similarity, and performance. We tested the framework’s impact on trust-building and trust learning using a mixed-methods approach, incorporating design tasks and structured workshops involving university students. The results highlight the framework’s ability to enhance participants’ understanding of trust in AI. Insights from this study contribute to advancing educational approaches for embedding trust in AI-driven design, revealing that design activities alone are not enough to impact trust learning.},
	journal = {Proceedings of the Design Society},
	author = {Lelli, Chiara and Chiarello, Filippo and Giordano, Vito},
	month = aug,
	year = {2025},
	pages = {971--980},
}

@book{hoffmann_psychologie_2025,
	address = {Berlin, Heidelberg},
	title = {Die {Psychologie} und die {Künstliche} {Intelligenz}: {Maschinen}, {Bewusstsein} und die menschliche {Psyche}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-70965-8 978-3-662-70966-5},
	shorttitle = {Die {Psychologie} und die {Künstliche} {Intelligenz}},
	url = {https://link.springer.com/10.1007/978-3-662-70966-5},
	language = {de},
	urldate = {2025-09-23},
	publisher = {Springer},
	author = {Hoffmann, Oliver},
	year = {2025},
	doi = {10.1007/978-3-662-70966-5},
	keywords = {Bewusstsein und Emotionen, Denksysteme im Wandel, Emotionen und KI, Ethik und Moral der KI, Ethik und Verantwortung, KI als Bedrohung, KI im Alltag, Künstliche Intelligenz, Mensch und Maschine, Mensch versus Maschine, Psychologie des Vertrauens, Psychologie mit und durch KI, Psychologische Mechanismen, Selbstbild und Identität, Zukunft der Psychologie, maschinelles Lernen, menschliches Denken},
}

@book{kopp_vertrauen_2022,
	title = {Vertrauen in {Roboter} und dessen {Beeinflussbarkeit} durch sprachliches {Framing}: {Eine} empirische {Untersuchung} der {Interaktion} mit {Cobots} am {Arbeitsplatz}},
	isbn = {978-3-7315-1206-6},
	shorttitle = {Vertrauen in {Roboter} und dessen {Beeinflussbarkeit} durch sprachliches {Framing}},
	url = {https://library.oapen.org/handle/20.500.12657/60477},
	abstract = {Collaborative robots (cobots) enable human-robot interactions in the workplace without safety fences. An appropriate level of trust by employees is critical to the success of these interactions. Anthropomorphic perceptions and fears of technological replacement affect trust formation. They can be influenced by linguistic framing, as this interdisciplinary empirical study shows.},
	language = {German},
	urldate = {2025-09-23},
	publisher = {KIT Scientific Publishing},
	author = {Kopp, Tobias},
	year = {2022},
	doi = {10.5445/KSP/1000146827},
	note = {Accepted: 2023-01-03T12:44:05Z},
	keywords = {Anthropomorphism, Anthropomorphismus, Collaborative robots, Human-robot interaction, Kollaborierende Roboter, Linguistic framing, Mensch-Roboter-Interaktion, Sprachliches Framing, Trust, Vertrauen, thema EDItEUR::N History and Archaeology::NH History},
}

@incollection{rotzel_kunstliche_2024,
	address = {Wiesbaden},
	title = {Künstliche {Intelligenz} ({KI}) – unser bester {Freund}?},
	isbn = {978-3-658-43816-6},
	url = {https://doi.org/10.1007/978-3-658-43816-6_2},
	abstract = {Künstliche Intelligenz (KI) hat sich zu einer transformativen Kraft entwickelt, die verschiedene Aspekte der täglichen Arbeit beeinflusst. Es stellt sich die Frage: Können Menschen freundschaftliche Beziehungen zu KI-Entscheidungsunterstützungssystemen aufbauen oder werden diese Systeme nur als Werkzeuge betrachtet? In diesem Kapitel werden die Dynamik, die Herausforderungen und die Möglichkeiten von Mensch-KI-Interaktionen (MKI) untersucht, wobei ein besonderer Fokus auf die entscheidende Rolle des Vertrauens in dieser Interaktion gelegt wird. Das Vertrauen in KI wird durch kognitive, emotionale und soziale Faktoren beeinflusst. Zu den kognitiven Faktoren gehören die Transparenz und Interpretierbarkeit von KI-Systemen, zu den emotionalen Faktoren gehören die emotionale Bindung und das Verhältnis zwischen Menschen und KI-Agenten und zu den sozialen Faktoren gehören gesellschaftliche Normen und kulturelle Einflüsse. Das Spannungsverhältnis zwischen Automatisierungs- und Algorithmusvermeidungstendenzen stellt eine komplexe Herausforderung für MKI dar. Automatisierungsbias bedeutet, sich unhinterfragt auf KI-Empfehlungen zu verlassen. Die Tendenz zur Algorithmusvermeidung beschreibt die Ablehnung oder das Übergehen von KI-Empfehlungen zugunsten eines menschlichen Urteils. Um dieses Spannungsfeld zu bewältigen, müssen transparente und erklärbare KI-Systeme entwickelt und eine effektive Zusammenarbeit zwischen Menschen und KI gefördert werden. Durch die Berücksichtigung dieser Faktoren und die Stärkung des Vertrauens kann MKI zu einer informierteren Entscheidungsfindung und einer effektiven Nutzung der KI-Funktionen führen.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Vertrauen in {Künstliche} {Intelligenz}: {Eine} multi-perspektivische {Betrachtung}},
	publisher = {Springer Fachmedien},
	author = {Rötzel, Peter Gordon},
	editor = {Schork, Sabrina},
	year = {2024},
	doi = {10.1007/978-3-658-43816-6_2},
	keywords = {Entscheidungsempfehlung, Entscheidungshilfesysteme, Künstliche Intelligenz, Vertrauen},
	pages = {19--33},
}

@book{schork_vertrauen_2024,
	address = {Wiesbaden},
	title = {Vertrauen in {Künstliche} {Intelligenz}: {Eine} multi-perspektivische {Betrachtung}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-658-43815-9 978-3-658-43816-6},
	shorttitle = {Vertrauen in {Künstliche} {Intelligenz}},
	url = {https://link.springer.com/10.1007/978-3-658-43816-6},
	language = {de},
	urldate = {2025-09-23},
	publisher = {Springer Fachmedien},
	editor = {Schork, Sabrina},
	year = {2024},
	doi = {10.1007/978-3-658-43816-6},
	keywords = {Anwendungsnahe Forschung, ChatGPT, Interdisziplinarität, Künstliche Intelligenz, Menschenzentrierte Künstliche Intelligenz, Vertrauen},
}

@incollection{weitz_vertrauen_2021,
	address = {Wiesbaden},
	title = {Vertrauen und {Vertrauenswürdigkeit} bei sozialen {Robotern}},
	isbn = {978-3-658-31114-8},
	url = {https://doi.org/10.1007/978-3-658-31114-8_16},
	abstract = {Dieses Kapitel befasst sich mit der Vertrauensbeziehung zwischen Menschen und sozialen Robotern, stellt doch Vertrauen einen wichtigen Bestandteil für die Akzeptanz sozialer Roboter dar. Ausgehend von den Merkmalen sozialer Interaktionen zwischen Mensch und Roboter wird ein Überblick über verschiedene Definitionen von Vertrauen in diesem Kontext gegeben. Zudem werden theoretische Vertrauensmodelle und praktische Möglichkeiten der Erfassung von Vertrauen skizziert sowie der Vertrauensverlust als Folge von Roboterfehlern betrachtet. Es wird beleuchtet, wie Erklärbare Künstliche Intelligenz helfen kann, eine transparente Interaktion zwischen Roboter und Mensch zu ermöglichen und dadurch das Vertrauen in soziale Roboter (wieder-)herzustellen. Insbesondere wird auf die Gestaltungsmöglichkeiten und Herausforderungen beim Einsatz von Erklärungen im Bereich der Robotik eingegangen. Die Wirkung, die Erklärungen von Robotern auf die mentalen Modelle von Nutzer:innen haben, bildet den Abschluss dieses Kapitels.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Soziale {Roboter}: {Technikwissenschaftliche}, wirtschaftswissenschaftliche, philosophische, psychologische und soziologische {Grundlagen}},
	publisher = {Springer Fachmedien},
	author = {Weitz, Katharina},
	editor = {Bendel, Oliver},
	year = {2021},
	doi = {10.1007/978-3-658-31114-8_16},
	pages = {309--323},
}

@incollection{tschopp_vertrauen_2022,
	address = {Berlin, Heidelberg},
	title = {Vertrauen {Sie} {KI}? {Einblicke} in das {Thema} {Künstliche} {Intelligenz} und warum {Vertrauen} eine {Schlüsselrolle} im {Umgang} mit neuen {Technologien} spielt},
	isbn = {978-3-662-63117-1},
	shorttitle = {Vertrauen {Sie} {KI}?},
	url = {https://doi.org/10.1007/978-3-662-63117-1_16},
	abstract = {No trust, no use? Oft wird Vertrauen als kritischer Erfolgsfaktor propagiert, wenn es um die Nutzung von neuen Technologien geht, vor allem wenn es sich um intelligente Systeme, sogenannte KI (Künstliche Intelligenz) handelt. Diese finden nämlich immer mehr Eingang in die heutige Gesellschaft, sowohl im privaten (z. B. Einkaufen mit Amazons KI-basiertem Smart Speaker Alexa) als auch im beruflichen oder schulischen Umfeld (z. B. intelligente Systeme, die die Personalauswahl oder Lernprozesse unterstützen sollen). Der Einsatz von smarten, ubiquitären Technologien erhöht die Unsicherheit und Skepsis, gerade bei EndanwenderInnen ohne technisches Verständnis, und verschärft das Spannungsfeld zwischen Mensch, Maschine und Gesellschaft. Die Vertrauensfrage wird ins Rampenlicht gerückt. Ist Vertrauen der KonsumentInnen die Lösung, um das hochgelobte Potenzial der künstlichen Intelligenz voll auszunutzen? Ganz so einfach ist es nicht. Unklarheiten in Definitionen, Sprachgebrauch und Messmethoden verwässern das Verständnis um die Zusammenhänge von Vertrauen und Nutzen. Es ist über die unterschiedlichen Disziplinen hinweg nicht eindeutig geklärt, ob die Nutzung von neuen Technologien, insbesondere KI, tatsächlich mit Vertrauen einhergeht, für welchen Zweck und Vertrauen in wen: Die HerstellerInnen? Die DesignerInnen? Wie kann Vertrauen und Nutzung abgegrenzt werden? Dieser Beitrag hat zum Ziel, kuriose Geschichten und Behauptungen rund um KI und Vertrauen zu entmystifizieren, um letztlich Künstliche Intelligenz besser in die Praxis zu bringen. Dazu braucht es einen transdisziplinären und vor allem adressatengerechten Diskurs darüber, was intelligente Systeme sind, und eine differenzierte Auseinandersetzung damit, welche Rolle Vertrauen dabei spielen könnte. Es gibt nämlich auch die Vertrauens-SkeptikerInnen, die vehement die Meinung vertreten, dass Vertrauen im Kontext KI überhaupt keine Rolle spielt. Wir argumentieren, dass Vertrauen – vor allem im Endanwenderkontext – eine wichtige Variable ist, welche nicht nur die Adoption, sondern auch die Art und Weise, wie KI-basierte Systeme genutzt werden, maßgeblich beeinflusst. Anhand von praktischen Beispielen wollen wir aufzeigen, dass ein angemessen kalibriertes Vertrauensniveau nicht nur zu einem effizienteren, sicheren und synergetischen Umgang mit KI-basierten oder automatisierten Systemen führt, sondern sogar Leben retten kann.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Kreativität und {Innovation} in {Organisationen} : {Impulse} aus {Innovationsforschung}, {Management}, {Kunst} und {Psychologie}},
	publisher = {Springer},
	author = {Tschopp, Marisa and Ruef, Marc and Monett, Dagmar},
	editor = {Landes, Miriam and Steiner, Eberhard and Utz, Tatjana},
	year = {2022},
	doi = {10.1007/978-3-662-63117-1_16},
	pages = {319--346},
}

@misc{schwartz_enhancing_2023,
	title = {Enhancing {Trust} in {LLM}-{Based} {AI} {Automation} {Agents}: {New} {Considerations} and {Future} {Challenges}},
	shorttitle = {Enhancing {Trust} in {LLM}-{Based} {AI} {Automation} {Agents}},
	url = {http://arxiv.org/abs/2308.05391},
	doi = {10.48550/arXiv.2308.05391},
	abstract = {Trust in AI agents has been extensively studied in the literature, resulting in significant advancements in our understanding of this field. However, the rapid advancements in Large Language Models (LLMs) and the emergence of LLM-based AI agent frameworks pose new challenges and opportunities for further research. In the field of process automation, a new generation of AI-based agents has emerged, enabling the execution of complex tasks. At the same time, the process of building automation has become more accessible to business users via user-friendly no-code tools and training mechanisms. This paper explores these new challenges and opportunities, analyzes the main aspects of trust in AI agents discussed in existing literature, and identifies specific considerations and challenges relevant to this new generation of automation agents. We also evaluate how nascent products in this category address these considerations. Finally, we highlight several challenges that the research community should address in this evolving landscape.},
	urldate = {2025-09-16},
	publisher = {arXiv},
	author = {Schwartz, Sivan and Yaeli, Avi and Shlomov, Segev},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05391 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{ding_citations_2025,
	title = {Citations and {Trust} in {LLM} {Generated} {Responses}},
	volume = {39},
	copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/34550},
	doi = {10.1609/aaai.v39i22.34550},
	abstract = {Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.},
	language = {en},
	number = {22},
	urldate = {2025-09-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ding, Yifan and Facciani, Matthew and Joyce, Ellen and Poudel, Amrit and Bhattacharya, Sanmitra and Veeramani, Balaji and Aguinaga, Sal and Weninger, Tim},
	month = apr,
	year = {2025},
	pages = {23787--23795},
}

@article{schlogl_chatbots_2024,
	series = {future internet 2024},
	title = {Chatbots in {Airport} {Customer} {Service}—{Exploring} {Use} {Cases} and {Technology} {Acceptance}},
	volume = {16},
	copyright = {CC BY 4.0},
	url = {https://www.researchgate.net/publication/380687481_Chatbots_in_Airport_Customer_Service-Exploring_Use_Cases_and_Technology_Acceptance},
	doi = {https://doi.org/10.3390/fi16050175},
	abstract = {Throughout the last decade, chatbots have gained widespread adoption across various industries, including healthcare, education, business, e-commerce, and entertainment. These types of artificial, usually cloud-based, agents have also been used in airport customer service, although there has been limited research concerning travelers’ perspectives on this rather techno-centric approach to handling inquiries. Consequently, the goal of the presented study was to tackle this research gap and explore potential use cases for chatbots at airports, as well as investigate travelers’ acceptance of said technology. We employed an extended version of the Technology Acceptance Model considering Perceived Usefulness, Perceived Ease of Use, Trust, and Perceived Enjoyment as predictors of Behavioral Intention, with Affinity for Technology as a potential moderator. A total of n=191 travelers completed our survey. The results show that Perceived Usefulness, Trust, Perceived Ease of Use, and Perceived Enjoyment positively correlate with the Behavioral Intention to use a chatbot for airport customer service inquiries, with Perceived Usefulness showing the highest impact. Travelers’ Affinity for Technology, on the other hand, does not seem to have any significant effect.},
	language = {en},
	number = {5},
	urldate = {2025-09-16},
	journal = {MDPI - Publisher of Open Access Journals},
	author = {Schlögl, Stephan and Auer, Isabelle and Gundula, Glowka},
	month = may,
	year = {2024},
	pages = {19},
}

@article{clegg_artificial_2024,
	title = {Artificial intelligence and management education: {A} conceptualization of human-machine interaction},
	volume = {22},
	issn = {1472-8117},
	shorttitle = {Artificial intelligence and management education},
	url = {https://www.sciencedirect.com/science/article/pii/S1472811724000788},
	doi = {10.1016/j.ijme.2024.101007},
	abstract = {The increasing use of Advanced Natural Language Processing (ANLP) models, particularly ChatGPT-4, presents opportunities and challenges to management education and research. These models can enhance the style, creativity, and analytical power of research papers, potentially shifting human scholars' roles from creators to ‘prompters’. If machines can perform educational and research tasks more effectively the role of human educators becomes a salient question in a world in which ANLP models offer clear, coherent, and polished insights, the use of which has potentially paradoxical possibilities. From one perspective, a new type of high-quality scholarship and education characterized by strong human involvement that synergistically leverages ANLP models' analytical capabilities, enabling human scholars to probe complex phenomena and make management research truly meaningful and impactful for broader audiences, is possible. We explore these questions through an ‘ideal type’ conceptualization of the possible relations between AI and management education and research.},
	number = {3},
	urldate = {2025-06-06},
	journal = {The International Journal of Management Education},
	author = {Clegg, Stewart and Sarkar, Soumodip},
	month = nov,
	year = {2024},
	keywords = {AI, Archetypes, ChatGPT, Management education},
	pages = {101007},
}

@article{li_developing_2024,
	title = {Developing trustworthy artificial intelligence: insights from research on interpersonal, human-automation, and human-{AI} trust},
	volume = {15},
	issn = {1664-1078},
	shorttitle = {Developing trustworthy artificial intelligence},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1382693/full},
	doi = {10.3389/fpsyg.2024.1382693},
	abstract = {The rapid advancement of artificial intelligence (AI) has impacted society in many aspects.Alongside this progress, concerns such as privacy violation, discriminatory bias, and safety risks have also surfaced, highlighting the need for the development of ethical, responsible, and socially beneficial AI. In response, the concept of trustworthy AI has gained prominence, and several guidelines for developing trustworthy AI have been proposed. Against this background, we demonstrate the significance of psychological research in identifying factors that contribute to the formation of trust in AI. Specifically, we review research findings on interpersonal, human-automation, and human-AI trust from the perspective of a threedimension framework (i.e., the trustor, the trustee, and their interactive context). The framework synthesizes common factors related to trust formation and maintenance across different trust types. These factors point out the foundational requirements for building trustworthy AI and provide pivotal guidance for its development that also involves communication, education, and training for users. We conclude by discussing how the insights in trust research can help enhance AI's trustworthiness and foster its adoption and application.},
	language = {English},
	urldate = {2025-06-03},
	journal = {Frontiers in Psychology},
	author = {Li, Yugang and Wu, Baizhou and Huang, Yuqi and Luan, Shenghua},
	month = apr,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {AI ethics, Human-Automation trust, Trustworthy AI, human-AI trust, interpersonal trust},
}

@article{riley_emotional_2024,
	title = {Emotional and cognitive trust in artificial intelligence: {A} framework for identifying research opportunities},
	volume = {58},
	issn = {2352-250X},
	shorttitle = {Emotional and cognitive trust in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S2352250X24000460},
	doi = {10.1016/j.copsyc.2024.101833},
	abstract = {This article briefly summarizes trust as a multi-dimensional construct, and trust in AI as a unique but related construct. It argues that because trust in AI is couched within an economic landscape, these two frameworks should be combined to understand the dynamics of trust in AI as it is currently implemented. The review focuses on healthcare and law enforcement as two industries that have adopted AI in ways that do and do not engender trust from stakeholders. The framework is applied to both industries to highlight where and why varying trust in AI is observed. Then seven research questions are posed, and researchers are encouraged to test the proposed framework in other AI-reliant contexts, like education and employment.},
	urldate = {2025-06-03},
	journal = {Current Opinion in Psychology},
	author = {Riley, Breagin K. and Dixon, Andrea},
	month = aug,
	year = {2024},
	keywords = {Artificial intelligence (AI), Healthcare, Law enforcement, Trust},
	pages = {101833},
}

@article{kuper_psychological_2025,
	title = {Psychological {Traits} and {Appropriate} {Reliance}: {Factors} {Shaping} {Trust} in {AI}},
	volume = {41},
	issn = {1044-7318},
	shorttitle = {Psychological {Traits} and {Appropriate} {Reliance}},
	url = {https://doi.org/10.1080/10447318.2024.2348216},
	doi = {10.1080/10447318.2024.2348216},
	abstract = {Research in AI-enabled decision support often focuses on technological factors influencing reliance on AI. However, the end-users of AI systems are individuals with diverse personalities which potentially lead to differences in collaborative human-computer interaction, resulting in harmful under- and over-reliance. The influence of psychological traits on appropriate reliance must be understood to enable development of more effective AI support addressing a diverse user base. This experimental mixed-methods study (N = 250) examined the impact of psychological traits on trust in and reliance on AI advice in classification tasks. Propensity to trust, affinity for technology interaction, and control beliefs in interacting with technology were identified as predictors for trust, which affect reliance. Thus, consideration must be given to the expected propensity to trust and the level of technological expertise among user groups when designing systems that aim to promote suitable degrees of trust and appropriate reliance.},
	number = {7},
	urldate = {2025-06-03},
	journal = {International Journal of Human–Computer Interaction},
	author = {Küper, Alisa and and Krämer, Nicole},
	month = apr,
	year = {2025},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2024.2348216},
	keywords = {AI reliance, appropriate reliance, decision support system, personality, trust},
	pages = {4115--4131},
}

@article{lukyanenko_trust_2022,
	title = {Trust in artificial intelligence: {From} a {Foundational} {Trust} {Framework} to emerging research opportunities},
	volume = {32},
	issn = {1422-8890},
	shorttitle = {Trust in artificial intelligence},
	url = {https://doi.org/10.1007/s12525-022-00605-4},
	doi = {10.1007/s12525-022-00605-4},
	abstract = {With the rise of artificial intelligence (AI), the issue of trust in AI emerges as a paramount societal concern. Despite increased attention of researchers, the topic remains fragmented without a common conceptual and theoretical foundation. To facilitate systematic research on this topic, we develop a Foundational Trust Framework to provide a conceptual, theoretical, and methodological foundation for trust research in general. The framework positions trust in general and trust in AI specifically as a problem of interaction among systems and applies systems thinking and general systems theory to trust and trust in AI. The Foundational Trust Framework is then used to gain a deeper understanding of the nature of trust in AI. From doing so, a research agenda emerges that proposes significant questions to facilitate further advances in empirical, theoretical, and design research on trust in AI.},
	language = {en},
	number = {4},
	urldate = {2025-06-03},
	journal = {Electronic Markets},
	author = {Lukyanenko, Roman and Maass, Wolfgang and Storey, Veda C.},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Artificial intelligence (AI), C71, C72, C73, C80, Computational Intelligence, D11, Explainable AI, Foundational Trust Framework, Intelligence Infrastructure, J00, L63, L64, L86, Logic in AI, Philosophy of Artificial Intelligence, Symbolic AI, Systems, Transparency, Trust, Trust in AI},
	pages = {1993--2020},
}

@article{kaplan_trust_2023,
	title = {Trust in {Artificial} {Intelligence}: {Meta}-{Analytic} {Findings}},
	volume = {65},
	issn = {0018-7208},
	shorttitle = {Trust in {Artificial} {Intelligence}},
	url = {https://doi.org/10.1177/00187208211013988},
	doi = {10.1177/00187208211013988},
	abstract = {ObjectiveThe present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.BackgroundThere are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.MethodData from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.ResultsResults showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.ConclusionOverall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.ApplicationFindings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.},
	language = {EN},
	number = {2},
	urldate = {2025-06-03},
	journal = {Human Factors},
	author = {Kaplan, Alexandra D. and Kessler, Theresa T. and Brill, J. Christopher and Hancock, P. A.},
	month = mar,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	pages = {337--359},
}

@article{abbass_social_2019,
	title = {Social {Integration} of {Artificial} {Intelligence}: {Functions}, {Automation} {Allocation} {Logic} and {Human}-{Autonomy} {Trust}},
	volume = {11},
	issn = {1866-9964},
	shorttitle = {Social {Integration} of {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s12559-018-9619-0},
	doi = {10.1007/s12559-018-9619-0},
	abstract = {Artificial intelligence (AI) is finding more uses in the human society resulting in a need to scrutinise the relationship between humans and AI. Technology itself has advanced from the mere encoding of human knowledge into a machine to designing machines that “know how” to autonomously acquire the knowledge they need, learn from it and act independently in the environment. Fortunately, this need is not new; it has scientific grounds that could be traced back to the inception of computers. This paper uses a multi-disciplinary lens to explore how the natural cognitive intelligence in a human could interface with the artificial cognitive intelligence of a machine. The scientific journey over the last 50 years will be examined to understand the Human-AI relationship, and to present the nature of, and the role of trust in, this relationship. Risks and opportunities sitting at the human-AI interface will be studied to reveal some of the fundamental technical challenges for a trustworthy human-AI relationship. The critical assessment of the literature leads to the conclusion that any social integration of AI into the human social system would necessitate a form of a relationship on one level or another in society, meaning that humans will “always” actively participate in certain decision-making loops—either in-the-loop or on-the-loop—that will influence the operations of AI, regardless of how sophisticated it is.},
	language = {en},
	number = {2},
	urldate = {2025-06-03},
	journal = {Cognitive Computation},
	author = {Abbass, Hussein A.},
	month = apr,
	year = {2019},
	keywords = {Adaptive aiding, Adaptive automation, Artificial Intelligence, Artificial intelligence, Augmented cognition, Automation logic, Cognitive cyber symbiosis, Function allocation, Human-AI teaming, Human-Machine Interfaces, Human-autonomy teaming, Humanitiy and Technology, Philosophy of Artificial Intelligence, Social Robotics, Symbolic AI, Trust},
	pages = {159--171},
}

@article{lockey_review_2021,
	title = {A {Review} of {Trust} in {Artificial} {Intelligence}: {Challenges}, {Vulnerabilities} and {Future} {Directions}},
	shorttitle = {A {Review} of {Trust} in {Artificial} {Intelligence}},
	url = {https://aisel.aisnet.org/hicss-54/os/trust/2},
	journal = {Hawaii International Conference on System Sciences 2021 (HICSS-54)},
	author = {Lockey, Steven and Gillespie, Nicole and Holm, Daniel and Someh, Ida Asadi},
	month = jan,
	year = {2021},
}

@article{sharan_effects_2020,
	title = {The effects of personality and locus of control on trust in humans versus artificial intelligence},
	volume = {6},
	issn = {2405-8440},
	url = {https://www.cell.com/heliyon/abstract/S2405-8440(20)31416-X},
	doi = {10.1016/j.heliyon.2020.e04572},
	language = {English},
	number = {8},
	urldate = {2025-06-03},
	journal = {Heliyon},
	author = {Sharan, Navya Nishith and Romano, Daniela Maria},
	month = aug,
	year = {2020},
	pmid = {32923706},
	note = {Publisher: Elsevier},
	keywords = {Artificial intelligence, Big five personality traits, Individual traits, Locus of control, Psychology, Trust},
}

@article{oksanen_trust_2020,
	title = {Trust {Toward} {Robots} and {Artificial} {Intelligence}: {An} {Experimental} {Approach} to {Human}–{Technology} {Interactions} {Online}},
	volume = {11},
	issn = {1664-1078},
	shorttitle = {Trust {Toward} {Robots} and {Artificial} {Intelligence}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.568256/full},
	doi = {10.3389/fpsyg.2020.568256},
	abstract = {Robotization and artificial intelligence (AI) are expected to change societies profoundly. Trust is an important factor of human–technology interactions, as robots and AI increasingly contribute to tasks previously handled by humans. Currently, there is a need for studies investigating trust toward AI and robots, especially in first-encounter meetings. This article reports findings from a study investigating trust toward robots and AI in an online trust game experiment. The trust game manipulated the hypothetical opponents that were described as either AI or robots. These were compared with control group opponents using only human name or nickname. Participants (N = 1077) lived in the United States. Describing opponents with robots or AI did not impact participants’ trust toward them. The robot called jdrx894 was the most trusted opponent. Opponents named “jdrx894” were trusted more than opponents called “Michael.” Further analysis showed that having a degree in technology or engineering, exposure to robots online and robot use self-efficacy predicted higher trust toward robots and AI. Out of Big Five personality characteristics, openness to experience predicted higher trust, and conscientiousness predicted lower trust. Results suggest trust on robots and AI is contextual and it also dependent on individual differences and knowledge on technology.},
	language = {English},
	urldate = {2025-06-03},
	journal = {Frontiers in Psychology},
	author = {Oksanen, Atte and Savela, Nina and Latikka, Rita and Koivula, Aki},
	month = dec,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Trust, artificial intelligence, human-technology interaction, individual differences, robot},
}


@article{grassi_knowledge-grounded_2022,
	title = {Knowledge-{Grounded} {Dialogue} {Flow} {Management} for {Social} {Robots} and {Conversational} {Agents}},
	volume = {14},
	doi = {10.1007/s12369-022-00868-z},
	abstract = {The article proposes a system for knowledge-based conversation designed for Social Robots and other conversational agents. The proposed system relies on an Ontology for the description of all concepts that may be relevant conversation topics, as well as their mutual relationships. The article focuses on the algorithm for Dialogue Management that selects the most appropriate conversation topic depending on the user input. Moreover, it discusses strategies to ensure a conversation flow that captures, as more coherently as possible , the user intention to drive the conversation in specific directions while avoiding purely reactive responses to what the user says. To measure the quality of the conversation, the article reports the tests performed with 100 recruited participants, comparing five conversational agents: (i) an agent addressing dialogue flow management based only on the detection of keywords in the speech, (ii) an agent based both on the detection of keywords and the Content Classification feature of Google Cloud Natural Language, (iii) an agent that picks conversation topics randomly, (iv) a human pretending to be a chatbot, and (v) one of the most famous chatbots worldwide: Replika. The subjective perception of the participants is measured both with the SASSI (Subjective Assessment of Speech System Interfaces) tool, as well as with a custom survey for measuring the subjective perception of coherence.},
	journal = {International Journal of Social Robotics},
	author = {Grassi, Lucrezia and Recchiuto, Carmine and Sgorbissa, Antonio},
	month = jul,
	year = {2022},
}

@article{kahr_understanding_2024,
	title = {Understanding {Trust} and {Reliance} {Development} in {AI} {Advice}: {Assessing} {Model} {Accuracy}, {Model} {Explanations}, and {Experiences} from {Previous} {Interactions}},
	volume = {14},
	issn = {2160-6455},
	shorttitle = {Understanding {Trust} and {Reliance} {Development} in {AI} {Advice}},
	url = {https://dl.acm.org/doi/10.1145/3686164},
	doi = {10.1145/3686164},
	abstract = {People are increasingly interacting with AI systems, but successful interactions depend on people trusting these systems only when appropriate. Since neither gaining trust in AI advice nor restoring lost trust after AI mistakes is warranted, we seek to better understand the development of trust and reliance in sequential human-AI interaction scenarios. In a 2  {\textbackslash}(\{{\textbackslash}times\}{\textbackslash})  2 between-subject simulated AI experiment, we tested how model accuracy (high vs. low) and explanation type (human-like vs. abstract) affect trust and reliance on AI advice for repeated interactions. In the experiment, participants estimated jail times for 20 criminal law cases, first without and then with AI advice. Our results show that trust and reliance are significantly higher for high model accuracy. In addition, reliance does not decline over the trial sequence, and trust increases significantly with high accuracy. Human-like (vs. abstract) explanations only increased reliance on the high-accuracy condition. We furthermore tested the extent to which trust and reliance in a trial round can be explained by trust and reliance experiences from prior rounds. We find that trust assessments in prior trials correlate with trust in subsequent ones. We also find that the cumulative trust experience of a person in all earlier trial rounds correlates with trust in subsequent ones. Furthermore, we find that the two trust measures, trust and reliance, impact each other: prior trust beliefs not only influence subsequent trust beliefs but likewise influence subsequent reliance behavior, and vice versa. Executing a replication study yielded comparable results to our original study, thereby enhancing the validity of our findings.},
	number = {4},
	urldate = {2025-11-04},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Kahr, Patricia K. and Rooks, Gerrit and Willemsen, Martijn C. and Snijders, Chris C. P.},
	month = dec,
	year = {2024},
	pages = {29:1--29:30},
}

@article{davis_perceived_1989,
	title = {Perceived {Usefulness}, {Perceived} {Ease} of {Use}, and {User} {Acceptance} of {Information} {Technology}},
	volume = {13},
	issn = {0276-7783},
	url = {https://www.jstor.org/stable/249008},
	doi = {10.2307/249008},
	abstract = {Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and stream-lined, resulting in two six-item scales with reliabilities of.98 for usefulness and.94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both self-reported current usage (r=.63, Study 1) and self-predicted future usage (r=.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r=.45, Study 1) and future usage (r=.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.},
	number = {3},
	urldate = {2025-11-04},
	journal = {MIS Quarterly},
	author = {Davis, Fred D.},
	year = {1989},
	note = {Publisher: Management Information Systems Research Center, University of Minnesota},
	pages = {319--340},
}

@article{davis_perceived_1989-1,
	title = {Perceived {Usefulness}, {Perceived} {Ease} of {Use}, and {User} {Acceptance} of {Information} {Technology}},
	volume = {13},
	issn = {0276-7783},
	url = {https://www.jstor.org/stable/249008},
	doi = {10.2307/249008},
	abstract = {Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and stream-lined, resulting in two six-item scales with reliabilities of.98 for usefulness and.94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both self-reported current usage (r=.63, Study 1) and self-predicted future usage (r=.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r=.45, Study 1) and future usage (r=.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.},
	number = {3},
	urldate = {2025-11-04},
	journal = {MIS Quarterly},
	author = {Davis, Fred D.},
	year = {1989},
	note = {Publisher: Management Information Systems Research Center, University of Minnesota},
	pages = {319--340},
}

@article{manresa_humanizing_2024,
	title = {Humanizing {GenAI} at work: bridging the gap between technological innovation and employee engagement},
	volume = {40},
	issn = {0268-3946},
	shorttitle = {Humanizing {GenAI} at work},
	url = {https://doi.org/10.1108/JMP-05-2024-0356},
	doi = {10.1108/JMP-05-2024-0356},
	abstract = {This paper seeks to explore the influence of generative artificial intelligence (GenAI) on employee performance in the workplace, viewed from a managerial perspective. It concentrates on key elements such as employee engagement, trust in GenAI and attitudes toward its implementation. This exploration is motivated by the ongoing evolution of GenAI, which presents managers with the crucial task of understanding and integrating this technology into their strategic frameworks.We collected 251 responses from managers and senior managers representing companies that have embraced GenAI in Spain. A hierarchical regression analysis was employed to examine the hypotheses. Subsequently, mediating effects and moderated mediation effects were scrutinized using the bias-corrected bootstrapping method.The data analysis suggests a significant enhancement in employee engagement and performance from a managerial perspective, attributed to improved attitudes and trust toward the adoption of GenAI. This conclusion is drawn from our research conducted with samples collected in Spain. Notably, our findings indicate that while positive attitudes toward GenAI correlate with enhanced engagement and performance, there exists a weakening effect on the significant positive impact of GenAI adoption in the workplace. This suggests that GenAI is still in its early stages of adoption within these companies, necessitating additional time for managers to develop greater confidence in its efficacy.This study represents one of the pioneering investigations centered on the implementation of GenAI within the workplace context. It contributes significantly to the existing body of literature concerning the stimulus-organism-response (S-O-R) model in technology innovation adoption within work environments.},
	number = {5},
	urldate = {2025-11-04},
	journal = {Journal of Managerial Psychology},
	author = {Manresa, Alba and Sammour, Ammar and Mas-Machuca, Marta and Chen, Weifeng and Botchie, David},
	month = aug,
	year = {2024},
	pages = {472--492},
}

@article{kaplan_trust_2023,
	title = {Trust in {Artificial} {Intelligence}: {Meta}-{Analytic} {Findings}},
	volume = {65},
	issn = {0018-7208},
	shorttitle = {Trust in {Artificial} {Intelligence}},
	url = {https://doi.org/10.1177/00187208211013988},
	doi = {10.1177/00187208211013988},
	abstract = {ObjectiveThe present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.BackgroundThere are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.MethodData from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.ResultsResults showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.ConclusionOverall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.ApplicationFindings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.},
	language = {EN},
	number = {2},
	urldate = {2025-11-04},
	journal = {Human Factors},
	author = {Kaplan, Alexandra D. and Kessler, Theresa T. and Brill, J. Christopher and Hancock, P. A.},
	month = mar,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	pages = {337--359},
}

@article{cao_understanding_2021,
	title = {Understanding managers’ attitudes and behavioral intentions towards using artificial intelligence for organizational decision-making},
	volume = {106},
	issn = {0166-4972},
	url = {https://www.sciencedirect.com/science/article/pii/S0166497221000936},
	doi = {10.1016/j.technovation.2021.102312},
	abstract = {While using artificial intelligence (AI) could improve organizational decision-making, it also creates challenges associated with the “dark side” of AI. However, there is a lack of research on managers' attitudes and intentions to use AI for decision making. To address this gap, we develop an integrated AI acceptance-avoidance model (IAAAM) to consider both the positive and negative factors that collectively influence managers' attitudes and behavioral intentions towards using AI. The research model is tested through a large-scale questionnaire survey of 269 UK business managers. Our findings suggest that IAAAM provides a more comprehensive model for explaining and predicting managers' attitudes and behavioral intentions towards using AI. Our research contributes conceptually and empirically to the emerging literature on using AI for organizational decision-making. Further, regarding the practical implications of using AI for organizational decision-making, we highlight the importance of developing favorable facilitating conditions, having an effective mechanism to alleviate managers’ personal concerns, and having a balanced consideration of both the benefits and the dark side associated with using AI.},
	urldate = {2025-10-21},
	journal = {Technovation},
	author = {Cao, Guangming and Duan, Yanqing and Edwards, John S. and Dwivedi, Yogesh K.},
	month = aug,
	year = {2021},
	keywords = {AI adoption, Artificial intelligence, Integrated AI acceptance-Avoidance model (IAAAM), Organizational decision-making, Technology threat avoidance theory (TTAT), Unified theory of acceptance and use of technology (UTAUT)},
	pages = {102312},
}

@article{gillath_attachment_2021,
	title = {Attachment and trust in artificial intelligence},
	volume = {115},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S074756322030354X},
	doi = {10.1016/j.chb.2020.106607},
	abstract = {Lack of trust is one of the main obstacles standing in the way of taking full advantage of the benefits artificial intelligence (AI) has to offer. Most research on trust in AI focuses on cognitive ways to boost trust. Here, instead, we focus on boosting trust in AI via affective means. Specifically, we tested and found associations between one's attachment style—an individual difference representing the way people feel, think, and behave in relationships—and trust in AI. In Study 1 we found that attachment anxiety predicted less trust. In Study 2, we found that enhancing attachment anxiety reduced trust, whereas enhancing attachment security increased trust in AI. In Study 3, we found that exposure to attachment security cues (but not positive affect cues) resulted in increased trust as compared with exposure to neutral cues. Overall, our findings demonstrate an association between attachment security and trust in AI, and support the ability to increase trust in AI via attachment security priming.},
	urldate = {2025-10-21},
	journal = {Computers in Human Behavior},
	author = {Gillath, Omri and Ai, Ting and Branicky, Michael S. and Keshmiri, Shawn and Davison, Robert B. and Spaulding, Ryan},
	month = feb,
	year = {2021},
	keywords = {Artificial intelligence, Attachment style, Close relationships, Trust},
	pages = {106607},
}

@article{chen_systematic_2025,
	title = {A {Systematic} {Review} of {User} {Attitudes} {Toward} {GenAI}: {Influencing} {Factors} and {Industry} {Perspectives}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-3200},
	shorttitle = {A {Systematic} {Review} of {User} {Attitudes} {Toward} {GenAI}},
	url = {https://www.mdpi.com/2079-3200/13/7/78},
	doi = {10.3390/jintelligence13070078},
	abstract = {In the era of GenAI, user attitude—shaped by cognition, emotion, and behavior—plays a critical role in the sustainable development of human–AI interaction. Human creativity and intelligence, as core drivers of social progress, are important factors influencing user attitudes. This paper systematically reviews 243 peer-reviewed studies on GenAI user attitudes published since 2019, identifying major research methods and theoretical perspectives, including the Technology Acceptance Model (TAM), the Unified Theory of Acceptance and Use of Technology (UTAUT), and the AI Device Use Acceptance (AIDUA) model. Drawing on contemporary creativity theories—such as Sternberg’s Theory of Successful Intelligence, the 4C Model by Kaufman and Beghetto, and the Dynamic Creativity Framework—we analyze how creativity and intelligence are conceptualized in current studies and how they affect user responses to GenAI. Through cross-cultural analysis and multimodal comparison, this review offers a comprehensive understanding of the interplay between GenAI and human creativity, aiming to support more inclusive and sustainable human–AI collaboration.},
	language = {en},
	number = {7},
	urldate = {2025-10-21},
	journal = {Journal of Intelligence},
	author = {Chen, Junjie and Xie, Wei and Xie, Qing and Hu, Anshu and Qiao, Yiran and Wan, Ruoyu and Liu, Yuhan},
	month = jul,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GenAI, acceptance, application domains, bibliometric analysis, creativity, intelligence, user attitudes},
	pages = {78},
}

@misc{noauthor_impact_nodate,
	title = {The {Impact} of {Familiarity} {Using} {Generative} {AI} to {Influence} {Intention} to {Use} {AI}},
	url = {https://ieeexplore.ieee.org/abstract/document/10730325},
	abstract = {AI has gained massive popularity in recent years with many tools coming out to be used and help people in daily life, especially AI writing tools. This scientific research investigates engagement and intention to use AI writing tools among students, from middle school to college. Despite the popularity, the study found that several students face issues while engaging with AI that problem can come from the complexity of the AI and the output of the AI, reducing their desire to use it. The research employs the Social Cognitive Theory (SCT) as its framework. Data were collected from 372 students, with 311 meeting the criteria for being processed using SmartPLS. The study's variables include familiarity, engagement (cognitive, behavioral, affective dimensions), and intention to use. The results indicate that five out of six hypotheses were accepted, with one hypothesis which is the relationship between familiarity and affective engagement being rejected. And the rest of the hypotheses are being accepted with a significant result},
	language = {en-US},
	urldate = {2025-10-21},
}

@article{topsakal_how_2025,
	title = {How {Familiarity}, {Ease} of {Use}, {Usefulness}, and {Trust} {Influence} the {Acceptance} of {Generative} {Artificial} {Intelligence} ({AI})-{Assisted} {Travel} {Planning}},
	volume = {41},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2024.2426044},
	doi = {10.1080/10447318.2024.2426044},
	abstract = {This study aimed to understand and analyze the factors influencing the usage of Gen AI-assisted travel planning, particularly Gen AI familiarity, perceived ease of use, perceived usefulness, and perceived trust. The dataset comprised 387 participants. The analysis was conducted using SmartPLS-4 and the SPSS software. The degree of familiarity with Gen AI affects perceived ease of use and perceived usefulness in the context of Gen AI-assisted travel planning. The perceived ease of use, in turn, affects the perceived usefulness of Gen AI-assisted travel planning. Furthermore, perceived usefulness directly affects the intention to use Gen AI-assisted travel planning. Familiarity with Gen AI also has a direct effect on perceived trust in Gen AI-assisted travel planning, which subsequently affects intention to use it. However, perceived ease of use does not have a direct effect on the intention to use Gen AI-assisted travel planning.},
	number = {15},
	urldate = {2025-10-21},
	journal = {International Journal of Human–Computer Interaction},
	author = {Topsakal, Yunus},
	month = aug,
	year = {2025},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2024.2426044},
	keywords = {Generative artificial ıntelligence, ease of use, familiarity, travel planning, trust, usefulness},
	pages = {9478--9491},
}

@misc{horowitz_adopting_2023,
	title = {Adopting {AI}: {How} {Familiarity} {Breeds} {Both} {Trust} and {Contempt}},
	shorttitle = {Adopting {AI}},
	url = {http://arxiv.org/abs/2305.01405},
	doi = {10.48550/arXiv.2305.01405},
	abstract = {Despite pronouncements about the inevitable diffusion of artificial intelligence and autonomous technologies, in practice it is human behavior, not technology in a vacuum, that dictates how technology seeps into -- and changes -- societies. In order to better understand how human preferences shape technological adoption and the spread of AI-enabled autonomous technologies, we look at representative adult samples of US public opinion in 2018 and 2020 on the use of four types of autonomous technologies: vehicles, surgery, weapons, and cyber defense. By focusing on these four diverse uses of AI-enabled autonomy that span transportation, medicine, and national security, we exploit the inherent variation between these AI-enabled autonomous use cases. We find that those with familiarity and expertise with AI and similar technologies were more likely to support all of the autonomous applications we tested (except weapons) than those with a limited understanding of the technology. Individuals that had already delegated the act of driving by using ride-share apps were also more positive about autonomous vehicles. However, familiarity cut both ways; individuals are also less likely to support AI-enabled technologies when applied directly to their life, especially if technology automates tasks they are already familiar with operating. Finally, opposition to AI-enabled military applications has slightly increased over time.},
	urldate = {2025-10-21},
	publisher = {arXiv},
	author = {Horowitz, Michael C. and Kahn, Lauren and Macdonald, Julia and Schneider, Jacquelyn},
	month = may,
	year = {2023},
	note = {arXiv:2305.01405 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@article{sindermann_assessing_2021,
	title = {Assessing the {Attitude} {Towards} {Artificial} {Intelligence}: {Introduction} of a {Short} {Measure} in {German}, {Chinese}, and {English} {Language}},
	volume = {35},
	issn = {1610-1987},
	shorttitle = {Assessing the {Attitude} {Towards} {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s13218-020-00689-0},
	doi = {10.1007/s13218-020-00689-0},
	abstract = {In the context of (digital) human–machine interaction, people are increasingly dealing with artificial intelligence in everyday life. Through this, we observe humans who embrace technological advances with a positive attitude. Others, however, are particularly sceptical and claim to foresee substantial problems arising from such uses of technology. The aim of the present study was to introduce a short measure to assess the Attitude Towards Artificial Intelligence (ATAI scale) in the German, Chinese, and English languages. Participants from Germany (N = 461; 345 females), China (N = 413; 145 females), and the UK (N = 84; 65 females) completed the ATAI scale, for which the factorial structure was tested and compared between the samples. Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica, which are representatives of popular artificial intelligence products. The results showed that the five-item ATAI scale comprises two negatively associated factors assessing (1) acceptance and (2) fear of artificial intelligence. The factor structure was found to be similar across the German, Chinese, and UK samples. Additionally, the ATAI scale was validated, as the items on the willingness to use specific artificial intelligence products were positively associated with the ATAI Acceptance scale and negatively with the ATAI Fear scale, in both the German and Chinese samples. In conclusion we introduce a short, reliable, and valid measure on the attitude towards artificial intelligence in German, Chinese, and English language.},
	language = {en},
	number = {1},
	urldate = {2025-10-21},
	journal = {KI - Künstliche Intelligenz},
	author = {Sindermann, Cornelia and Sha, Peng and Zhou, Min and Wernicke, Jennifer and Schmitt, Helena S. and Li, Mei and Sariyska, Rayna and Stavrou, Maria and Becker, Benjamin and Montag, Christian},
	month = mar,
	year = {2021},
	keywords = {ATAI, ATAI scale, Artificial Intelligence, Attitude Towards Artificial Intelligence, Chinese, English, German},
	pages = {109--118},
}

@article{schepman_general_2023,
	title = {The {General} {Attitudes} towards {Artificial} {Intelligence} {Scale} ({GAAIS}): {Confirmatory} {Validation} and {Associations} with {Personality}, {Corporate} {Distrust}, and {General} {Trust}},
	volume = {39},
	issn = {1044-7318},
	shorttitle = {The {General} {Attitudes} towards {Artificial} {Intelligence} {Scale} ({GAAIS})},
	url = {https://doi.org/10.1080/10447318.2022.2085400},
	doi = {10.1080/10447318.2022.2085400},
	abstract = {Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public’s attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.},
	number = {13},
	urldate = {2025-10-21},
	journal = {International Journal of Human–Computer Interaction},
	author = {Schepman, Astrid and Rodway, Paul},
	month = aug,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2022.2085400},
	pages = {2724--2741},
}

@article{ibrahim_technology_2025,
	title = {The technology acceptance model and adopter type analysis in the context of artificial intelligence},
	volume = {7},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1496518/full},
	doi = {10.3389/frai.2024.1496518},
	abstract = {IntroductionArtificial Intelligence (AI) is a transformative technology impacting various sectors of society and the economy. Understanding the factors influencing AI adoption is critical for both research and practice. This study focuses on two key objectives: (1) validating an extended version of the Technology Acceptance Model (TAM) in the context of AI by integrating the Big Five personality traits and AI mindset, and (2) conducting an exploratory k-prototype analysis to classify AI adopters based on demographics, AI-related attitudes, and usage patterns.MethodsA sample of N = 1,007 individuals individuals (60\% female; M = 30.92; SD = 8.63 years) was collected. Psychometric data were obtained using validated scales for TAM constructs, Big Five personality traits, and AI mindset. Regression analysis was used to validate TAM, and a k-prototype clustering algorithm was applied to classify participants into adopter categories.ResultsThe psychometric analysis confirmed the validity of the extended TAM. Perceived usefulness was the strongest predictor of attitudes towards AI usage (β = 0.34, p {\textless} 0.001), followed by AI mindset scale growth (β = 0.28, p {\textless} 0.001). Additionally, openness was positively associated with perceived ease of use (β = 0.15, p {\textless} 0.001). The k-prototype analysis revealed four distinct adopter clusters, consistent with the diffusion of innovations model: early adopters (n = 218), early majority (n = 331), late majority (n = 293), and laggards (n = 165).DiscussionThe findings highlight the importance of perceived usefulness and AI mindset in shaping attitudes toward AI adoption. The clustering results provide a nuanced understanding of AI adopter types, aligning with established innovation diffusion theories. Implications for AI deployment strategies, policy-making, and future research directions are discussed.},
	language = {English},
	urldate = {2025-10-21},
	journal = {Frontiers in Artificial Intelligence},
	author = {Ibrahim, Fabio and Münscher, Johann-Christoph and Daseking, Monika and Telle, Nils-Torge},
	month = jan,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {AI mindset, artificial Intelligence, big five, early adopter, late adopter, technology acceptance model},
}

@article{baron_moderator-mediator_1986,
	title = {The moderator-mediator variable distinction in social psychological research: {Conceptual}, strategic, and statistical considerations},
	volume = {51},
	shorttitle = {The moderator-mediator variable distinction in social psychological research},
	doi = {10.1037//0022-3514.51.6.1173},
	abstract = {In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators. (46 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
	journal = {Journal of Personality and Social Psychology},
	author = {Baron, Reuben and Kenny, David},
	month = jan,
	year = {1986},
	pages = {1173--1182},
}

@incollection{nunnally_psychometric_1994,
	address = {New York},
	edition = {3rd},
	series = {{McGraw}-{Hill} series in psychology},
	title = {Psychometric {Theory}},
	copyright = {McGraw-Hill},
	isbn = {978-0-07-047849-7},
	url = {https://search.library.berkeley.edu/discovery/fulldisplay/alma991055658409706532/01UCS_BER:UCB},
	abstract = {Like the previous edition, this text is designed as a comprehensive text in measurement for researchers and for use in graduate courses in psychology, education and areas of business such as management and marketing. It is intended to consider the broad measurement problems that arise in these areas and is written for a reader who needs only a basic background in statistics to comprehend the material. It also combines classical procedures that explain variance with modern inferential procedures.},
	language = {en},
	booktitle = {Psychometric {Theory}},
	publisher = {McGraw-Hill},
	author = {Nunnally, Jum C. and Bernstein, Ira H.},
	year = {1994},
	pages = {752},
}

@misc{nunnally_psychometric_nodate,
	title = {Psychometric theory / {Jum} {C}. {Nunnally}, {Ira} {H}. {Bernstein}. - {University} of {California} {Berkeley}},
	url = {https://search.library.berkeley.edu/discovery/fulldisplay/alma991055658409706532/01UCS_BER:UCB},
	abstract = {(Publisher-supplied data) The classic text is Psychometric Theory. Like the previous edition, this text is designed as a comprehensive text in measurement for researchers and for use in graduate courses in psychology, education and areas of business such as management and marketing. It is intended to consider the broad measurement problems that arise in these areas and is written for a reader who needs only a basic background in statistics to comprehend the material. It also combines classical procedures that explain variance with modern inferential procedures.},
	language = {en},
	urldate = {2025-10-07},
	author = {Nunnally, Jum C. and Bernstein, Ira H.},
}

@incollection{gade_konfirmatorische_2020,
	title = {Konfirmatorische {Faktorenanalyse} ({CFA})},
	isbn = {978-3-662-61531-7},
	abstract = {Das Kapitel bietet eine Einführung in die Grundlagen der konfirmatorischen Faktorenanalyse (CFA). Im Rahmen der modernen Testkonstruktion stellt die CFA ein wichtiges Instrument zur Überprüfung der Dimensionalität und damit der faktoriellen Validität eines Tests dar. So können die theoretischen Annahmen eines Modells wie die Anzahl der Faktoren und die Zuordnung der Testitems zu den Faktoren explizit als Hypothesen aufgestellt und getestet werden. In diesem Kapitel werden theoretische Bezüge der CFA zur Klassischen Testtheorie (KTT) hergestellt, praktische Aspekte der Hypothesenbildung, Modellspezifikation und -identifikation behandelt sowie ein kurzer Überblick über Schätzverfahren und Gütekriterien zur Modellevaluation gegeben. Die CFA wird für ausgewählte ein- und mehrdimensionale Modelle an einem empirischen Beispiel vorgestellt. Der Einsatz der CFA zur Überprüfung der Messäquivalenz von Items, die für die Reliabilitätsschätzung von Bedeutung ist, werden ebenso besprochen wie Möglichkeiten des Modellvergleichs, der Modellmodifikation und der Überprüfung der Messinvarianz eines Tests über verschiedene Gruppen oder Messzeitpunkte hinweg.},
	author = {Gäde, Jana C. and Schermelleh-Engel, Karin and Brandt, Holger},
	month = aug,
	year = {2020},
	doi = {10.1007/978-3-662-61532-4_24},
	pages = {615--659},
}

@book{joreskog_lisrel_1981,
	title = {{LISREL} {V}: {Analysis} of linear structural relationships by maximum likelihood and least squares methods.},
	copyright = {Scientific Software},
	language = {en},
	publisher = {University of Uppsala},
	author = {Jöreskog, Karl G. and Sörbom, Dag},
	year = {1981},
}

@article{anderson_structural_1988,
	title = {Structural equation modeling in practice: {A} review and recommended two-step approach},
	volume = {103},
	issn = {1939-1455},
	shorttitle = {Structural equation modeling in practice},
	doi = {10.1037/0033-2909.103.3.411},
	abstract = {In this article, we provide guidance for substantive researchers on the use of structural equation modeling in practice for theory testing and development. We present a comprehensive, two-step modeling approach that employs a series of nested models and sequential chi-square difference tests. We discuss the comparative advantages of this approach over a one-step approach. Considerations in specification, assessment of fit, and respecification of measurement models using confirmatory factor analysis are reviewed. As background to the two-step approach, the distinction between exploratory and confirmatory analysis, the distinction between complementary approaches for theory testing versus predictive application, and some developments in estimation methods also are discussed. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Anderson, James C. and Gerbing, David W.},
	year = {1988},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Confirmatory Factor Analysis, Factor Analysis, Mathematical Modeling, Statistical Analysis},
	pages = {411--423},
}

@techreport{bentler_eqs_1995,
	address = {Enico, California CA},
	type = {Manual},
	title = {{EQS} 6 {Structural} equations program manual},
	url = {https://www3.nd.edu/~kyuan/courses/sem/EQS-Manual6.pdf},
	language = {en},
	number = {6},
	urldate = {2025-07-10},
	institution = {Multivariate Software, Inc.},
	author = {Bentler, Peter M.},
	year = {1995},
	pages = {418},
}

@article{hu_cutoff_1999,
	title = {Cutoff criteria for fit indexes in covariance structure analysis: {Conventional} criteria versus new alternatives},
	volume = {6},
	issn = {1532-8007},
	shorttitle = {Cutoff criteria for fit indexes in covariance structure analysis},
	doi = {10.1080/10705519909540118},
	abstract = {Examines the adequacy of the "rules of thumb" conventional cutoff criteria and several new alternatives for various fit indexes used to evaluate model fit in practice. Using a 2-index presentation strategy, which includes using the maximum likelihood (ML)-based standardized root mean squared residual (SRMR) and supplementing it with either Tucker-Lewis Index (TLI), Bollen's (1989) Fit Index (BL89), Relative Noncentrality Index (RNI), Comparative Fit Index (CFI), Gamma Hat, McDonald's Centrality Index (Mc), or root mean squared error of approximation (RMSEA), various combinations of cutoff values from selected ranges of cutoff criteria for the ML-based SRMR and a given supplemental fit index were used to calculate rejection rates for various types of true-population and misspecified models, models with misspecified factor covariance and models with misspecified factor loading. The results suggest that, for the ML method, a cutoff value close to .95 for TLI, BL89, CFI, RNI, and Gamma Hat; a value close to .90 for Mc; a cutoff value close to .08 for SRMR; and one close to .06 for RMSEA are needed before it can be concluded that there is a good fit between the hypothesized model and the observed data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Structural Equation Modeling},
	author = {Hu, Li-tze and Bentler, Peter M.},
	year = {1999},
	note = {Place: US
Publisher: Lawrence Erlbaum},
	keywords = {Factor Analysis, Goodness of Fit, Maximum Likelihood},
	pages = {1--55},
}

@article{browne_alternative_1992,
	title = {Alternative {Ways} of {Assessing} {Model} {Fit}},
	volume = {21},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124192021002005},
	doi = {10.1177/0049124192021002005},
	abstract = {This article is concerned with measures of fit of a model. Two types of error involved in fitting a model are considered. The first is error of approximation which involves the fit of the model, with optimally chosen but unknown parameter values, to the population covariance matrix. The second is overall error which involves the fit of the model, with parameter values estimated from the sample, to the population covariance matrix. Measures of the two types of error are proposed and point and interval estimates of the measures are suggested. These measures take the number of parameters in the model into account in order to avoid penalizing parsimonious models. Practical difficulties associated with the usual tests of exact fit or a model are discussed and a test of “close fit” of a model is suggested.},
	language = {EN},
	number = {2},
	urldate = {2025-10-07},
	journal = {Sociological Methods \& Research},
	author = {Browne, Michael W. and Cudeck, Robert},
	month = nov,
	year = {1992},
	note = {Publisher: SAGE Publications Inc},
	pages = {230--258},
}

@article{steiger_statistically-based_1980,
	title = {Statistically-based tests for the number of factors ({Handout})},
	volume = {23},
	issn = {1532-8007},
	doi = {10.1080/10705511.2016.1217487},
	abstract = {This note presents the original Steiger-Lind (1980) handout entitled, “Statistically-Based Tests for the Number of Common Factors”, distributed to all in attendance at the talk given at the annual meeting of the Psychometric Society in Iowa City, Iowa. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {6},
	journal = {Structural Equation Modeling},
	author = {Steiger, James H. and Lind, J.C.},
	year = {1980},
	note = {Place: United Kingdom
Publisher: Taylor \& Francis},
	keywords = {Models, Psychometrics, Simulation, Statistics},
	pages = {777--781},
}

@article{tucker_reliability_1973,
	title = {A reliability coefficient for maximum likelihood factor analysis},
	volume = {38},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02291170},
	doi = {10.1007/BF02291170},
	abstract = {Maximum likelihood factor analysis provides an effective method for estimation of factor matrices and a useful test statistic in the likelihood ratio for rejection of overly simple factor models. A reliability coefficient is proposed to indicate quality of representation of interrelations among attributes in a battery by a maximum likelihood factor analysis. Usually, for a large sample of individuals or objects, the likelihood ratio statistic could indicate that an otherwise acceptable factor model does not exactly represent the interrelations among the attributes for a population. The reliability coefficient could indicate a very close representation in this case and be a better indication as to whether to accept or reject the factor solution.},
	language = {en},
	number = {1},
	urldate = {2025-10-07},
	journal = {Psychometrika},
	author = {Tucker, Ledyard R. and Lewis, Charles},
	month = mar,
	year = {1973},
	keywords = {Common Factor, Factor Model, Factor Solution, Likelihood Ratio Statistic, Reliability Coefficient},
	pages = {1--10},
}

@article{bentler_comparative_1990,
	title = {Comparative fit indexes in structural models},
	volume = {107},
	issn = {1939-1455},
	url = {https://psycnet.apa.org/record/1990-13755-001},
	doi = {10.1037/0033-2909.107.2.238},
	abstract = {Normed and nonnormed fit indexes are frequently used as adjuncts to chi-square statistics for evaluating the fit of a structural model. A drawback of existing indexes is that they estimate no known population parameters. A new coefficient is proposed to summarize the relative reduction in the noncentrality parameters of 2 nested models. Two estimators of the coefficient yield new normed (CFIN) and nonnormed (FIN) fit indexes. CFIN avoids the underestimation of fit often noted in small samples for P. M. Bentler and D. G. Bonett's (see record 1981-06898-001) normed fit index (NFIN). FIN is a linear function of Bentler and Bonett's nonnormed fit index (NNFIN) that avoids the extreme underestimation and overestimation often found in NNFIN. Asymptotically, CFIN, FIN, NFIN, and a new index developed by K. A. Bollen (1989) are equivalent measures of comparative fit, whereas NNFIN measures relative fit by comparing noncentrality per degree of freedom. All of the indexes are generalized to permit use of Wald and Lagrange multiplier statistics. An example illustrates the behavior of these indexes under conditions of correct specification and misspecification. The new fit indexes perform very well at all sample sizes. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Bulletin},
	author = {Bentler, P. M.},
	year = {1990},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Goodness of Fit, Methodology, Models, Population (Statistics), Statistical Estimation, Statistical Norms},
	pages = {238--246},
}

@article{kano_software_1997,
	title = {Software},
	volume = {24},
	issn = {1349-6964},
	url = {https://doi.org/10.2333/bhmk.24.85},
	doi = {10.2333/bhmk.24.85},
	abstract = {Eleven CAS program authors were invited to write a description of their own program and answer the questionnaire enclosed. We received seven responses. Following their descriptions, the answers to the questionnaire are summarized in tables. Additional information on CSA programs is found in Waller (1993, Applied Psychological Measurement, Vol. 17, pp. 73-100) and Howell (1996, Journal of Marketing Research, Vol. 33, pp. 377-381).},
	language = {en},
	number = {1},
	urldate = {2025-10-07},
	journal = {Behaviormetrika},
	author = {Kano, Yutaka and Arbuckle, James L. and McDonald, Roderick P. and Fraser, Colin and Bentier, Peter M. and Jöreskog, Karl G. and Arminger, G. and Browne, Michael W. and Steiger, James H.},
	month = jan,
	year = {1997},
	pages = {85--125},
}

@incollection{tversky_framing_1986,
	series = {Logic, {Methodology} and {Philosophy} of {Science} {VII}},
	title = {The {Framing} of {Decisions} and the {Evaluation} of {Prospects}},
	volume = {114},
	url = {https://www.sciencedirect.com/science/article/pii/S0049237X09707104},
	abstract = {The chapter presents a series of demonstrations in which seemingly inconsequential changes in the formulation of choice problems caused significant shifts of preference. The inconsistencies were traced to the interaction of two sets of factors: variation in the framing of acts, contingencies and outcomes, and the characteristic non-linearities of values and decision weights. The demonstrated effects are large and systematic, although by no means universal. They occur when the outcomes concern the loss of human lives as well as in choices about money; they are not restricted to hypothetical questions and are not eliminated by monetary incentives. The chapter is concerned primarily with the descriptive question of how decisions are made and emphasizes that the psychology of choice is also relevant to the normative question of how decisions ought to be made. The framing of acts and outcomes can also reflect the acceptance or rejection of responsibility for particular consequences, and the deliberate manipulation of framing is commonly used as an instrument of self-control. When framing influences the experience of consequences, the adoption of a decision frame is an ethically significant act.},
	language = {en},
	urldate = {2025-09-29},
	booktitle = {Studies in {Logic} and the {Foundations} of {Mathematics}},
	publisher = {Elsevier},
	author = {Tversky, Amos and Kahneman, Daniel},
	editor = {Barcan Marcus, Ruth and Dorn, Georg J. W. and Weingartner, Paul},
	month = jan,
	year = {1986},
	doi = {10.1016/S0049-237X(09)70710-4},
	pages = {503--520},
}

@article{freling_when_2014,
	title = {When not to accentuate the positive: {Re}-examining valence effects in attribute framing},
	volume = {124},
	issn = {0749-5978},
	shorttitle = {When \textit{not} to accentuate the positive},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597814000090},
	doi = {10.1016/j.obhdp.2013.12.007},
	abstract = {While the expanding body of attribute framing literature provides keen insights into individual judgments and evaluations, a lack of theoretical perspective inhibits scholars from more fully extending research foci beyond a relatively straightforward examination of message content. The current research applies construal level theory to attribute framing research. The authors conduct a meta-analysis of 107 published articles and then conceptually expand this knowledge base by synthesizing attribute framing research and construal level concepts. Results suggest that attribute framing is most effective when there is congruence between the construal level evoked in a frame and the evaluator’s psychological distance from the framed event. A follow-up experiment confirms that the congruence between a frame’s construal level and psychological distance—not simply its valence—appears to be driving attribute framing effects. This research proposes to shift the focus in attribute framing research from that of message composition to a more complex relationship between the message and the recipient.},
	number = {2},
	urldate = {2025-09-28},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Freling, Traci H. and Vincent, Leslie H. and Henard, David H.},
	month = jul,
	year = {2014},
	keywords = {Construal level theory, Framing effects, Message framing, Meta-analysis},
	pages = {95--109},
}

@misc{cunningham_how_2025,
	address = {Cambridge, MA 02138},
	title = {How {People} {Use} {ChatGPT}},
	copyright = {National Bureau of Economic Research, 1050 Massachusetts Avenue, Cambridge, MA 02138},
	shorttitle = {How {People} {Use} {ChatGPT}},
	url = {https://www.nber.org/papers/w34255},
	doi = {10.3386/w34255},
	language = {en},
	urldate = {2025-09-28},
	author = {Cunningham, Thomas and Deming, J. David and Hitzig, Zoe and Ong, Christopher and Yan Shan, Carl and Wadman, Kevin},
	month = sep,
	year = {2025},
}

@article{baroni_ai-tam_2022,
	title = {{AI}-{TAM}: a model to investigate user acceptance and collaborative intention in human-in-the-loop {AI} applications},
	volume = {9},
	copyright = {Copyright (c) 2022 Ilaria Baroni, Gloria Re Calegari, Damiano Scandolari, Irene Celino},
	issn = {2330-8001},
	shorttitle = {{AI}-{TAM}},
	url = {https://hcjournal.org/index.php/jhc/article/view/134},
	doi = {10.15346/hc.v9i1.134},
	abstract = {More and more frequently, digital applications make use of Artificial Intelligence (AI) capabilities to provide advanced features; on the other hand, human-in-the-loop approaches are on the rise to involve people in AI-powered pipelines for data collection, results validation and decision-making.
Does the introduction of AI features affect user acceptance? Does the AI result quality affect people willingness to use such applications? Does the additional user effort required in human-in-the-loop mechanisms change the application adoption and use?
This study aims to provide a reference approach to answer those questions. We propose a model that extends the Technology Acceptance Model (TAM) with further constructs explicitly related to AI (user trust in AI and perceived quality of AI output, from XAI literature) and collaborative intention (willingness to contribute to AI pipelines).
We tested the proposed model with an application for car damage claim reporting with AI-powered damage estimation for insurance customers. The results showed that the XAI related factors have a strong and positive effect on the behavioural intention, the perceived usefulness and the ease of use of the application. Moreover, there is a strong link between the behavioural intention and the collaborative intention, indicating that indeed human-in-the-loop approaches can be successfully adopted in final user applications.},
	language = {en},
	number = {1},
	urldate = {2025-09-28},
	journal = {Human Computation},
	author = {Baroni, Ilaria and Calegari, Gloria Re and Scandolari, Damiano and Celino, Irene},
	month = may,
	year = {2022},
	pages = {1--21},
}

@article{ray_can_2019,
	title = {Can {You} {Explain} {That}? {Lucid} {Explanations} {Help} {Human}-{AI} {Collaborative} {Image} {Retrieval}},
	volume = {7},
	shorttitle = {Can {You} {Explain} {That}?},
	doi = {10.1609/hcomp.v7i1.5275},
	abstract = {While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as “helpful” significantly improve human performance, “incorrect” and “unhelpful” explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanation on a human-AI collaborative task.},
	journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	author = {Ray, Arijit and Yao, Yi and Kumar, Rakesh and Divakaran, Ajay and Burachas, Giedrius},
	month = oct,
	year = {2019},
	pages = {153--161},
}

@misc{singh_generative_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Generative {AI} through the {Lens} of {Technology} {Acceptance} {Model}},
	url = {https://papers.ssrn.com/abstract=4953174},
	doi = {10.2139/ssrn.4953174},
	abstract = {This paper applies the Technology Acceptance Model (TAM) to explore the adoption of Generative AI systems like ChatGPT, Claude, Gemini, and Copilot. It analyzes how Perceived Usefulness (PU) and Perceived Ease of Use (PEOU) influence the increasing adoption of these technologies. The study finds that simplifying technical complexity enhances users' perceptions of utility and ease, boosting adoption intentions. Key factors include effective organizational training, intuitive design, and strategic partnerships. The paper also addresses challenges such as trust and ethical concerns, offering insights into user acceptance in the rapidly evolving AI landscape.},
	language = {en},
	urldate = {2025-09-23},
	publisher = {Social Science Research Network},
	author = {Singh, Dr Preet Deep},
	month = sep,
	year = {2024},
	keywords = {Dr Preet Deep Singh, Generative AI through the Lens of Technology Acceptance Model, SSRN},
}

@article{budnik_can_2025,
	title = {Can {We} {Trust} {Artificial} {Intelligence}?},
	volume = {38},
	doi = {10.1007/s13347-024-00820-1},
	abstract = {In view of the dramatic advancements in the development of artificial intelligence technology in recent years, it has become a commonplace to demand that AI systems be trustworthy. This view presupposes that it is possible to trust AI technology in the first place. The aim of this paper is to challenge this view. In order to do that, it is argued that the philosophy of trust really revolves around the problem of how to square the epistemic and the normative dimensions of trust. Given this double nature of trust it is possible to extract a threefold challenge to the defenders of the possibility of AI trust without presupposing any particular trust theory. They have to show (1) how trust in AI systems is more than mere reliance; (2) how AI systems can become objects of normative expectations; and (3) how the resulting attitude gives human agents reassurance in their interactions with AI systems. In order to demonstrate how difficult this task is, the threefold challenge is then applied to two recent accounts that defend the possibility of trust in AI systems. By way of conclusion it is suggested that instead of trusting AI systems, we should strive to make them reliable.},
	journal = {Philosophy \& Technology},
	author = {Budnik, Christian},
	month = jan,
	year = {2025},
}

@article{kim_communicating_2022,
	title = {Communicating the {Limitations} of {AI}: {The} {Effect} of {Message} {Framing} and {Ownership} on {Trust} in {Artificial} {Intelligence}},
	volume = {39},
	shorttitle = {Communicating the {Limitations} of {AI}},
	doi = {10.1080/10447318.2022.2049134},
	abstract = {Trust plays an essential role in the interaction between humans and artificial intelligence (AI). To promote trust in AI, information about the AI’s performance should be communicated well to the users. Accordingly, this paper investigates how information about AI performance should be presented, focusing on message framing and the ownership of decisions. A 2 (ownership: no ownership vs. ownership) × 3 (message framing: no information vs. negative information vs. positive information) between-subjects experiment was conducted (N = 120). Participants were asked to choose items to help them survive in the desert, supported by an AI decision. The results showed that participants without decision ownership perceived higher trust than those with decision ownership. Also, trust was perceived to be higher when participants were not given performance information than when they were. The results indicate the importance of carefully communicating with AI. The implications of this study are discussed.},
	journal = {International Journal of Human-Computer Interaction},
	author = {Kim, Taenyun and Song, Hayeon},
	month = apr,
	year = {2022},
	pages = {1--11},
}

@article{ding_citations_2025,
	title = {Citations and {Trust} in {LLM} {Generated} {Responses}},
	volume = {39},
	copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/34550},
	doi = {10.1609/aaai.v39i22.34550},
	abstract = {Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.},
	language = {en},
	number = {22},
	urldate = {2025-09-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ding, Yifan and Facciani, Matthew and Joyce, Ellen and Poudel, Amrit and Bhattacharya, Sanmitra and Veeramani, Balaji and Aguinaga, Sal and Weninger, Tim},
	month = apr,
	year = {2025},
	pages = {23787--23795},
}

@article{dolgopolova_effect_2022,
	title = {The effect of attribute framing on consumers’ attitudes and intentions toward food: {A} {Meta}-analysis},
	volume = {10},
	shorttitle = {The effect of attribute framing on consumers’ attitudes and intentions toward food},
	doi = {10.36253/bae-11511},
	abstract = {This paper analyzes the existing literature on the effect of attribute framing on consumers’ attitudes and intentions with regard to food products. Attribute framing includes a broader interpretation of gains and losses when a product attribute is presented in a dichotomous way, such as fat vs. lean or harm vs. benefit. Meta-analysis results for the whole sample indicate that product attributes framed as gains have a higher effect on attitudes and intentions than product attributes framed as losses. Grouping studies by outcome variables, the meta-analysis demonstrates a larger effect size for studies that assess consumer attitude while for studies dealing with consumer intention, the effect size is close to zero and insignificant. We observe from the meta-regression results that the gain frame, the use of interaction terms, a specific product, and a student sample significantly influence consumers’ attitudes and intentions.},
	journal = {Bio-based and Applied Economics},
	author = {Dolgopolova, Irina and Li, Bingqing and Pirhonen, Helena and Roosen, Jutta},
	month = mar,
	year = {2022},
	pages = {253--264},
}

@book{bollen_structural_1989,
	address = {11 River Street, Hoboken, New Jersey, 07030-5774, USA},
	title = {Structural {Equations} with {Latent} {Variables}},
	copyright = {John Wiley \& Sons, Inc},
	isbn = {978-1-118-61917-9},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118619179},
	abstract = {Analysis of Ordinal Categorical Data Alan Agresti Statistical Science Now has its first coordinated manual of methods for analyzing ordered categorical data. This book discusses specialized models that, unlike standard methods underlying nominal categorical data, efficiently use the information on ordering. It begins with an introduction to basic descriptive and inferential methods for categorical data, and then gives thorough coverage of the most current developments, such as loglinear and logit models for ordinal data. Special emphasis is placed on interpretation and application of methods and contains an integrated comparison of the available strategies for analyzing ordinal data.},
	language = {en},
	urldate = {2025-07-10},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Bollen, Kenneth A.},
	year = {1989},
}

@book{fishbein_belief_1975,
	title = {Belief, attitude, intention and behaviour: {An} introduction to theory and research},
	volume = {27},
	shorttitle = {Belief, attitude, intention and behaviour},
	author = {Fishbein, M. and Ajzen, Icek},
	month = may,
	year = {1975},
}

@article{richey_jr_artificial_2023,
	title = {Artificial intelligence in logistics and supply chain management: {A} primer and roadmap for research},
	volume = {44},
	copyright = {© 2023 Wiley Periodicals LLC.},
	issn = {2158-1592},
	shorttitle = {Artificial intelligence in logistics and supply chain management},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jbl.12364},
	doi = {10.1111/jbl.12364},
	abstract = {The dawn of generative artificial intelligence (AI) has the potential to transform logistics and supply chain management radically. However, this promising innovation is met with a scholarly discourse grappling with an interplay between the promising capabilities and potential drawbacks. This conversation frequently includes dystopian forecasts of mass unemployment and detrimental repercussions concerning academic research integrity. Despite the current hype, existing research exploring the intersection between AI and the logistics and supply chain management (L\&SCM) sector remains limited. Therefore, this editorial seeks to fill this void, synthesizing the potential applications of AI within the L\&SCM domain alongside an analysis of the implementation challenges. In doing so, we propose a robust research framework as a primer and roadmap for future research. This will give researchers and organizations comprehensive insights and strategies to navigate the complex yet promising landscape of AI integration within the L\&SCM domain.},
	language = {en},
	number = {4},
	urldate = {2025-10-06},
	journal = {Journal of Business Logistics},
	author = {Richey Jr., Robert Glenn and Chowdhury, Soumyadeb and Davis-Sramek, Beth and Giannakis, Mihalis and Dwivedi, Yogesh K.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jbl.12364},
	keywords = {ChatGPT, LLMs, generative AI, generative artificial intelligence, large language models, logistics management, supply chain management},
	pages = {532--549},
}

@article{ajzen_bayesian_1975,
	title = {A {Bayesian} analysis of attribution processes},
	volume = {82},
	doi = {10.1037/h0076477},
	abstract = {Argues that Bayes's theorem is applicable to an analysis of attribution processes. Causal attribution is equated with the likelihood ratio and should therefore be accompanied by revisions in subjective probabilities or beliefs. A review of empirical research supports this view by showing that factors found to influence attribution are also related to the likelihood ratio in Bayes's theorem. Differences in attributions between actor and observer, as well as tendencies toward nonrational attribution, are also discussed from the point of view of a Bayesian information processing model. (53 ref)},
	journal = {Psychological Bulletin},
	author = {Ajzen, Icek and Fishbein, Martin},
	month = mar,
	year = {1975},
	pages = {261--277},
}

@article{druckman_evaluating_2001,
	title = {Evaluating framing effects},
	volume = {22},
	issn = {0167-4870},
	url = {https://www.sciencedirect.com/science/article/pii/S0167487000000325},
	doi = {10.1016/S0167-4870(00)00032-5},
	abstract = {This paper examines Tversky and Kahneman's well-known Asian disease framing problem (A. Tversky, D. Kahneman, Science 211 (1981) 453–458). I describe an experiment where respondents received a version of the disease problem using a survival format, a mortality format, or both formats. The results from the survival and mortality formats replicate Tversky and Kahneman's original experiment both in terms of statistical significance and, in contrast to some other studies, in terms of magnitude. I then argue that the “both format” condition constitutes an important and previously unused baseline for evaluating the strength of framing effects. This standard of comparison provides a way to evaluate the impact of a frame on unadulterated preferences – that is, preferences unaffected by a particular frame. The implications for future framing effect experiments are discussed.},
	number = {1},
	urldate = {2025-09-28},
	journal = {Journal of Economic Psychology},
	author = {Druckman, James N.},
	month = feb,
	year = {2001},
	keywords = {Framing effect},
	pages = {91--101},
}

@article{li_dimensions_2020,
	title = {Dimensions of artificial intelligence anxiety based on the integrated fear acquisition theory},
	volume = {63},
	issn = {0160-791X},
	url = {https://www.sciencedirect.com/science/article/pii/S0160791X20300476},
	doi = {10.1016/j.techsoc.2020.101410},
	abstract = {With the rapid development of artificial intelligence (AI), AI anxiety has emerged and is receiving widespread attention, but research on this topic is not comprehensive. Therefore, we investigated the dimensions of AI anxiety using the theoretical model of integrated fear acquisition and a questionnaire survey. A total of 494 valid questionnaires were recovered. Through a first-order confirmatory factor analysis (CFA), a factor model of AI anxiety was constructed, and eight factors of AI anxiety were verified. Then, a second-order CFA was applied to verify the adaptation of the factor structure of AI anxiety to fear acquisition. We identified four dimensions of AI anxiety and proposed a theory of AI anxiety acquisition that illustrates four pathways of AI anxiety acquisition. Each pathway includes two factors that cause AI anxiety. We conclude by analyzing the limitations of current AI anxiety research and proposing a broader research agenda for AI anxiety.},
	urldate = {2025-09-28},
	journal = {Technology in Society},
	author = {Li, Jian and Huang, Jin-Song},
	month = nov,
	year = {2020},
	keywords = {Artificial intelligence, Artificial intelligence anxiety, Factor model of AI anxiety, Integrated fear acquisition theory},
	pages = {101410},
}


@article{grassi_knowledge-grounded_2022,
	title = {Knowledge-{Grounded} {Dialogue} {Flow} {Management} for {Social} {Robots} and {Conversational} {Agents}},
	volume = {14},
	doi = {10.1007/s12369-022-00868-z},
	abstract = {The article proposes a system for knowledge-based conversation designed for Social Robots and other conversational agents. The proposed system relies on an Ontology for the description of all concepts that may be relevant conversation topics, as well as their mutual relationships. The article focuses on the algorithm for Dialogue Management that selects the most appropriate conversation topic depending on the user input. Moreover, it discusses strategies to ensure a conversation flow that captures, as more coherently as possible , the user intention to drive the conversation in specific directions while avoiding purely reactive responses to what the user says. To measure the quality of the conversation, the article reports the tests performed with 100 recruited participants, comparing five conversational agents: (i) an agent addressing dialogue flow management based only on the detection of keywords in the speech, (ii) an agent based both on the detection of keywords and the Content Classification feature of Google Cloud Natural Language, (iii) an agent that picks conversation topics randomly, (iv) a human pretending to be a chatbot, and (v) one of the most famous chatbots worldwide: Replika. The subjective perception of the participants is measured both with the SASSI (Subjective Assessment of Speech System Interfaces) tool, as well as with a custom survey for measuring the subjective perception of coherence.},
	journal = {International Journal of Social Robotics},
	author = {Grassi, Lucrezia and Recchiuto, Carmine and Sgorbissa, Antonio},
	month = jul,
	year = {2022},
}

@article{kahr_understanding_2024,
	title = {Understanding {Trust} and {Reliance} {Development} in {AI} {Advice}: {Assessing} {Model} {Accuracy}, {Model} {Explanations}, and {Experiences} from {Previous} {Interactions}},
	volume = {14},
	issn = {2160-6455},
	shorttitle = {Understanding {Trust} and {Reliance} {Development} in {AI} {Advice}},
	url = {https://dl.acm.org/doi/10.1145/3686164},
	doi = {10.1145/3686164},
	abstract = {People are increasingly interacting with AI systems, but successful interactions depend on people trusting these systems only when appropriate. Since neither gaining trust in AI advice nor restoring lost trust after AI mistakes is warranted, we seek to better understand the development of trust and reliance in sequential human-AI interaction scenarios. In a 2  {\textbackslash}(\{{\textbackslash}times\}{\textbackslash})  2 between-subject simulated AI experiment, we tested how model accuracy (high vs. low) and explanation type (human-like vs. abstract) affect trust and reliance on AI advice for repeated interactions. In the experiment, participants estimated jail times for 20 criminal law cases, first without and then with AI advice. Our results show that trust and reliance are significantly higher for high model accuracy. In addition, reliance does not decline over the trial sequence, and trust increases significantly with high accuracy. Human-like (vs. abstract) explanations only increased reliance on the high-accuracy condition. We furthermore tested the extent to which trust and reliance in a trial round can be explained by trust and reliance experiences from prior rounds. We find that trust assessments in prior trials correlate with trust in subsequent ones. We also find that the cumulative trust experience of a person in all earlier trial rounds correlates with trust in subsequent ones. Furthermore, we find that the two trust measures, trust and reliance, impact each other: prior trust beliefs not only influence subsequent trust beliefs but likewise influence subsequent reliance behavior, and vice versa. Executing a replication study yielded comparable results to our original study, thereby enhancing the validity of our findings.},
	number = {4},
	urldate = {2025-11-04},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Kahr, Patricia K. and Rooks, Gerrit and Willemsen, Martijn C. and Snijders, Chris C. P.},
	month = dec,
	year = {2024},
	pages = {29:1--29:30},
}

@article{zhai_effects_2024,
	title = {The effects of over-reliance on {AI} dialogue systems on students' cognitive abilities: a systematic review},
	volume = {11},
	issn = {2196-7091},
	shorttitle = {The effects of over-reliance on {AI} dialogue systems on students' cognitive abilities},
	url = {https://doi.org/10.1186/s40561-024-00316-7},
	doi = {10.1186/s40561-024-00316-7},
	abstract = {The growing integration of artificial intelligence (AI) dialogue systems within educational and research settings highlights the importance of learning aids. Despite examination of the ethical concerns associated with these technologies, there is a noticeable gap in investigations on how these ethical issues of AI contribute to students’ over-reliance on AI dialogue systems, and how such over-reliance affects students’ cognitive abilities. Overreliance on AI occurs when users accept AI-generated recommendations without question, leading to errors in task performance in the context of decision-making. This typically arises when individuals struggle to assess the reliability of AI or how much trust to place in its suggestions. This systematic review investigates how students’ over-reliance on AI dialogue systems, particularly those embedded with generative models for academic research and learning, affects their critical cognitive capabilities including decision-making, critical thinking, and analytical reasoning. By using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our systematic review evaluated a body of literature addressing the contributing factors and effects of such over-reliance within educational and research contexts. The comprehensive literature review spanned 14 articles retrieved from four distinguished databases: ProQuest, IEEE Xplore, ScienceDirect, and Web of Science. Our findings indicate that over-reliance stemming from ethical issues of AI impacts cognitive abilities, as individuals increasingly favor fast and optimal solutions over slow ones constrained by practicality. This tendency explains why users prefer efficient cognitive shortcuts, or heuristics, even amidst the ethical issues presented by AI technologies.},
	number = {1},
	urldate = {2025-11-04},
	journal = {Smart Learning Environments},
	author = {Zhai, Chunpeng and Wibowo, Santoso and Li, Lily D.},
	month = jun,
	year = {2024},
	keywords = {Analytical thinking, Cognitive abilities, Critical thinking, Decision-making, Ethical issues of AI, Generative AI},
	pages = {28},
}

@article{davis_perceived_1989,
	title = {Perceived {Usefulness}, {Perceived} {Ease} of {Use}, and {User} {Acceptance} of {Information} {Technology}},
	volume = {13},
	issn = {0276-7783},
	url = {https://www.jstor.org/stable/249008},
	doi = {10.2307/249008},
	abstract = {Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and stream-lined, resulting in two six-item scales with reliabilities of.98 for usefulness and.94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both self-reported current usage (r=.63, Study 1) and self-predicted future usage (r=.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r=.45, Study 1) and future usage (r=.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.},
	number = {3},
	urldate = {2025-11-04},
	journal = {MIS Quarterly},
	author = {Davis, Fred D.},
	year = {1989},
	note = {Publisher: Management Information Systems Research Center, University of Minnesota},
	pages = {319--340},
}

@article{manresa_humanizing_2024,
	title = {Humanizing {GenAI} at work: bridging the gap between technological innovation and employee engagement},
	volume = {40},
	issn = {0268-3946},
	shorttitle = {Humanizing {GenAI} at work},
	url = {https://doi.org/10.1108/JMP-05-2024-0356},
	doi = {10.1108/JMP-05-2024-0356},
	abstract = {This paper seeks to explore the influence of generative artificial intelligence (GenAI) on employee performance in the workplace, viewed from a managerial perspective. It concentrates on key elements such as employee engagement, trust in GenAI and attitudes toward its implementation. This exploration is motivated by the ongoing evolution of GenAI, which presents managers with the crucial task of understanding and integrating this technology into their strategic frameworks.We collected 251 responses from managers and senior managers representing companies that have embraced GenAI in Spain. A hierarchical regression analysis was employed to examine the hypotheses. Subsequently, mediating effects and moderated mediation effects were scrutinized using the bias-corrected bootstrapping method.The data analysis suggests a significant enhancement in employee engagement and performance from a managerial perspective, attributed to improved attitudes and trust toward the adoption of GenAI. This conclusion is drawn from our research conducted with samples collected in Spain. Notably, our findings indicate that while positive attitudes toward GenAI correlate with enhanced engagement and performance, there exists a weakening effect on the significant positive impact of GenAI adoption in the workplace. This suggests that GenAI is still in its early stages of adoption within these companies, necessitating additional time for managers to develop greater confidence in its efficacy.This study represents one of the pioneering investigations centered on the implementation of GenAI within the workplace context. It contributes significantly to the existing body of literature concerning the stimulus-organism-response (S-O-R) model in technology innovation adoption within work environments.},
	number = {5},
	urldate = {2025-11-04},
	journal = {Journal of Managerial Psychology},
	author = {Manresa, Alba and Sammour, Ammar and Mas-Machuca, Marta and Chen, Weifeng and Botchie, David},
	month = aug,
	year = {2024},
	pages = {472--492},
}

@article{kaplan_trust_2023,
	title = {Trust in {Artificial} {Intelligence}: {Meta}-{Analytic} {Findings}},
	volume = {65},
	issn = {0018-7208},
	shorttitle = {Trust in {Artificial} {Intelligence}},
	url = {https://doi.org/10.1177/00187208211013988},
	doi = {10.1177/00187208211013988},
	abstract = {ObjectiveThe present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.BackgroundThere are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.MethodData from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.ResultsResults showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.ConclusionOverall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.ApplicationFindings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.},
	language = {EN},
	number = {2},
	urldate = {2025-11-04},
	journal = {Human Factors},
	author = {Kaplan, Alexandra D. and Kessler, Theresa T. and Brill, J. Christopher and Hancock, P. A.},
	month = mar,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	pages = {337--359},
}

@misc{noauthor_evaluating_nodate,
	title = {Evaluating {Human}-{AI} {Collaboration}: {A} {Review} and {Methodological} {Framework}},
	url = {https://arxiv.org/html/2407.19098v1},
	urldate = {2025-10-21},
}

@article{cao_understanding_2021,
	title = {Understanding managers’ attitudes and behavioral intentions towards using artificial intelligence for organizational decision-making},
	volume = {106},
	issn = {0166-4972},
	url = {https://www.sciencedirect.com/science/article/pii/S0166497221000936},
	doi = {10.1016/j.technovation.2021.102312},
	abstract = {While using artificial intelligence (AI) could improve organizational decision-making, it also creates challenges associated with the “dark side” of AI. However, there is a lack of research on managers' attitudes and intentions to use AI for decision making. To address this gap, we develop an integrated AI acceptance-avoidance model (IAAAM) to consider both the positive and negative factors that collectively influence managers' attitudes and behavioral intentions towards using AI. The research model is tested through a large-scale questionnaire survey of 269 UK business managers. Our findings suggest that IAAAM provides a more comprehensive model for explaining and predicting managers' attitudes and behavioral intentions towards using AI. Our research contributes conceptually and empirically to the emerging literature on using AI for organizational decision-making. Further, regarding the practical implications of using AI for organizational decision-making, we highlight the importance of developing favorable facilitating conditions, having an effective mechanism to alleviate managers’ personal concerns, and having a balanced consideration of both the benefits and the dark side associated with using AI.},
	urldate = {2025-10-21},
	journal = {Technovation},
	author = {Cao, Guangming and Duan, Yanqing and Edwards, John S. and Dwivedi, Yogesh K.},
	month = aug,
	year = {2021},
	keywords = {AI adoption, Artificial intelligence, Integrated AI acceptance-Avoidance model (IAAAM), Organizational decision-making, Technology threat avoidance theory (TTAT), Unified theory of acceptance and use of technology (UTAUT)},
	pages = {102312},
}

@article{gillath_attachment_2021,
	title = {Attachment and trust in artificial intelligence},
	volume = {115},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S074756322030354X},
	doi = {10.1016/j.chb.2020.106607},
	abstract = {Lack of trust is one of the main obstacles standing in the way of taking full advantage of the benefits artificial intelligence (AI) has to offer. Most research on trust in AI focuses on cognitive ways to boost trust. Here, instead, we focus on boosting trust in AI via affective means. Specifically, we tested and found associations between one's attachment style—an individual difference representing the way people feel, think, and behave in relationships—and trust in AI. In Study 1 we found that attachment anxiety predicted less trust. In Study 2, we found that enhancing attachment anxiety reduced trust, whereas enhancing attachment security increased trust in AI. In Study 3, we found that exposure to attachment security cues (but not positive affect cues) resulted in increased trust as compared with exposure to neutral cues. Overall, our findings demonstrate an association between attachment security and trust in AI, and support the ability to increase trust in AI via attachment security priming.},
	urldate = {2025-10-21},
	journal = {Computers in Human Behavior},
	author = {Gillath, Omri and Ai, Ting and Branicky, Michael S. and Keshmiri, Shawn and Davison, Robert B. and Spaulding, Ryan},
	month = feb,
	year = {2021},
	keywords = {Artificial intelligence, Attachment style, Close relationships, Trust},
	pages = {106607},
}

@article{wen_trust_2025,
	title = {Trust and {AI} weight: human-{AI} collaboration in organizational management decision-making},
	volume = {3},
	issn = {2813-771X},
	shorttitle = {Trust and {AI} weight},
	url = {https://www.frontiersin.org/journals/organizational-psychology/articles/10.3389/forgp.2025.1419403/full},
	doi = {10.3389/forgp.2025.1419403},
	abstract = {IntroductionThe emergence of Artificial Intelligence (AI) has revolutionized decision-making in human resource management. Since human and AI each possesses distinct strengths in the realm of decision-making, the synergy between human and AI agent has the potential to significantly enhance both the efficiency and the quality of managerial decision-making processes. Although assigning decision weights to AI agents presents innovative avenues for human-AI collaboration, the underlying mechanisms driving the allocation of decision weights to AI agents remain inadequately understood. To elucidate these mechanisms, this paper examines the influence of trust in AI on AI weight allocation within the framework of human-AI cooperation, leveraging the Socio-Cognitive Model of Trust (SCMT).MethodsWe conducted a series of survey studies involving scenario-based decision-making tasks. Study 1 examined the relationship between trust in AI and AI weight among 111 managers about employee recruitment tasks. Study 2 surveyed 210 managers using employee performance evaluation tasks.ResultsThe results of Study 1 indicated that trust in AI enhances the decisional weight attributed to AI agents, and willingness to collaborate with AI mediates trust in AI and the weight of AI in personnel selection. The findings of Study 2 revealed that the perceived free will of AI agents negatively moderates the relationship between trust in AI and willing to collaborate with AI, such that the relationship is weaker when individuals perceive a higher degree of free will in AI agents than a lower degree.DiscussionTheoretically, this paper advances the understanding of the function of trust in human-AI interaction by exploring the trust development from attitude to act in human-AI cooperative decision-making. Practically, it offers valuable insights into the design of AI agent and organizational management within the context of human-AI collaboration.},
	language = {English},
	urldate = {2025-10-21},
	journal = {Frontiers in Organizational Psychology},
	author = {Wen, Yanjun and Wang, Jiale and Chen, Xiaoxi},
	month = jun,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {AI weight, decision-making, human-AI cooperation, socio-cognitive model of trust, trust},
}

@article{chen_systematic_2025,
	title = {A {Systematic} {Review} of {User} {Attitudes} {Toward} {GenAI}: {Influencing} {Factors} and {Industry} {Perspectives}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-3200},
	shorttitle = {A {Systematic} {Review} of {User} {Attitudes} {Toward} {GenAI}},
	url = {https://www.mdpi.com/2079-3200/13/7/78},
	doi = {10.3390/jintelligence13070078},
	abstract = {In the era of GenAI, user attitude—shaped by cognition, emotion, and behavior—plays a critical role in the sustainable development of human–AI interaction. Human creativity and intelligence, as core drivers of social progress, are important factors influencing user attitudes. This paper systematically reviews 243 peer-reviewed studies on GenAI user attitudes published since 2019, identifying major research methods and theoretical perspectives, including the Technology Acceptance Model (TAM), the Unified Theory of Acceptance and Use of Technology (UTAUT), and the AI Device Use Acceptance (AIDUA) model. Drawing on contemporary creativity theories—such as Sternberg’s Theory of Successful Intelligence, the 4C Model by Kaufman and Beghetto, and the Dynamic Creativity Framework—we analyze how creativity and intelligence are conceptualized in current studies and how they affect user responses to GenAI. Through cross-cultural analysis and multimodal comparison, this review offers a comprehensive understanding of the interplay between GenAI and human creativity, aiming to support more inclusive and sustainable human–AI collaboration.},
	language = {en},
	number = {7},
	urldate = {2025-10-21},
	journal = {Journal of Intelligence},
	author = {Chen, Junjie and Xie, Wei and Xie, Qing and Hu, Anshu and Qiao, Yiran and Wan, Ruoyu and Liu, Yuhan},
	month = jul,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GenAI, acceptance, application domains, bibliometric analysis, creativity, intelligence, user attitudes},
	pages = {78},
}

@misc{noauthor_impact_nodate,
	title = {The {Impact} of {Familiarity} {Using} {Generative} {AI} to {Influence} {Intention} to {Use} {AI}},
	url = {https://ieeexplore.ieee.org/abstract/document/10730325},
	abstract = {AI has gained massive popularity in recent years with many tools coming out to be used and help people in daily life, especially AI writing tools. This scientific research investigates engagement and intention to use AI writing tools among students, from middle school to college. Despite the popularity, the study found that several students face issues while engaging with AI that problem can come from the complexity of the AI and the output of the AI, reducing their desire to use it. The research employs the Social Cognitive Theory (SCT) as its framework. Data were collected from 372 students, with 311 meeting the criteria for being processed using SmartPLS. The study's variables include familiarity, engagement (cognitive, behavioral, affective dimensions), and intention to use. The results indicate that five out of six hypotheses were accepted, with one hypothesis which is the relationship between familiarity and affective engagement being rejected. And the rest of the hypotheses are being accepted with a significant result},
	language = {en-US},
	urldate = {2025-10-21},
}

@article{topsakal_how_2025,
	title = {How {Familiarity}, {Ease} of {Use}, {Usefulness}, and {Trust} {Influence} the {Acceptance} of {Generative} {Artificial} {Intelligence} ({AI})-{Assisted} {Travel} {Planning}},
	volume = {41},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2024.2426044},
	doi = {10.1080/10447318.2024.2426044},
	abstract = {This study aimed to understand and analyze the factors influencing the usage of Gen AI-assisted travel planning, particularly Gen AI familiarity, perceived ease of use, perceived usefulness, and perceived trust. The dataset comprised 387 participants. The analysis was conducted using SmartPLS-4 and the SPSS software. The degree of familiarity with Gen AI affects perceived ease of use and perceived usefulness in the context of Gen AI-assisted travel planning. The perceived ease of use, in turn, affects the perceived usefulness of Gen AI-assisted travel planning. Furthermore, perceived usefulness directly affects the intention to use Gen AI-assisted travel planning. Familiarity with Gen AI also has a direct effect on perceived trust in Gen AI-assisted travel planning, which subsequently affects intention to use it. However, perceived ease of use does not have a direct effect on the intention to use Gen AI-assisted travel planning.},
	number = {15},
	urldate = {2025-10-21},
	journal = {International Journal of Human–Computer Interaction},
	author = {Topsakal, Yunus},
	month = aug,
	year = {2025},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2024.2426044},
	keywords = {Generative artificial ıntelligence, ease of use, familiarity, travel planning, trust, usefulness},
	pages = {9478--9491},
}

@misc{horowitz_adopting_2023,
	title = {Adopting {AI}: {How} {Familiarity} {Breeds} {Both} {Trust} and {Contempt}},
	shorttitle = {Adopting {AI}},
	url = {http://arxiv.org/abs/2305.01405},
	doi = {10.48550/arXiv.2305.01405},
	abstract = {Despite pronouncements about the inevitable diffusion of artificial intelligence and autonomous technologies, in practice it is human behavior, not technology in a vacuum, that dictates how technology seeps into -- and changes -- societies. In order to better understand how human preferences shape technological adoption and the spread of AI-enabled autonomous technologies, we look at representative adult samples of US public opinion in 2018 and 2020 on the use of four types of autonomous technologies: vehicles, surgery, weapons, and cyber defense. By focusing on these four diverse uses of AI-enabled autonomy that span transportation, medicine, and national security, we exploit the inherent variation between these AI-enabled autonomous use cases. We find that those with familiarity and expertise with AI and similar technologies were more likely to support all of the autonomous applications we tested (except weapons) than those with a limited understanding of the technology. Individuals that had already delegated the act of driving by using ride-share apps were also more positive about autonomous vehicles. However, familiarity cut both ways; individuals are also less likely to support AI-enabled technologies when applied directly to their life, especially if technology automates tasks they are already familiar with operating. Finally, opposition to AI-enabled military applications has slightly increased over time.},
	urldate = {2025-10-21},
	publisher = {arXiv},
	author = {Horowitz, Michael C. and Kahn, Lauren and Macdonald, Julia and Schneider, Jacquelyn},
	month = may,
	year = {2023},
	note = {arXiv:2305.01405 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@article{sindermann_assessing_2021,
	title = {Assessing the {Attitude} {Towards} {Artificial} {Intelligence}: {Introduction} of a {Short} {Measure} in {German}, {Chinese}, and {English} {Language}},
	volume = {35},
	issn = {1610-1987},
	shorttitle = {Assessing the {Attitude} {Towards} {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s13218-020-00689-0},
	doi = {10.1007/s13218-020-00689-0},
	abstract = {In the context of (digital) human–machine interaction, people are increasingly dealing with artificial intelligence in everyday life. Through this, we observe humans who embrace technological advances with a positive attitude. Others, however, are particularly sceptical and claim to foresee substantial problems arising from such uses of technology. The aim of the present study was to introduce a short measure to assess the Attitude Towards Artificial Intelligence (ATAI scale) in the German, Chinese, and English languages. Participants from Germany (N = 461; 345 females), China (N = 413; 145 females), and the UK (N = 84; 65 females) completed the ATAI scale, for which the factorial structure was tested and compared between the samples. Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica, which are representatives of popular artificial intelligence products. The results showed that the five-item ATAI scale comprises two negatively associated factors assessing (1) acceptance and (2) fear of artificial intelligence. The factor structure was found to be similar across the German, Chinese, and UK samples. Additionally, the ATAI scale was validated, as the items on the willingness to use specific artificial intelligence products were positively associated with the ATAI Acceptance scale and negatively with the ATAI Fear scale, in both the German and Chinese samples. In conclusion we introduce a short, reliable, and valid measure on the attitude towards artificial intelligence in German, Chinese, and English language.},
	language = {en},
	number = {1},
	urldate = {2025-10-21},
	journal = {KI - Künstliche Intelligenz},
	author = {Sindermann, Cornelia and Sha, Peng and Zhou, Min and Wernicke, Jennifer and Schmitt, Helena S. and Li, Mei and Sariyska, Rayna and Stavrou, Maria and Becker, Benjamin and Montag, Christian},
	month = mar,
	year = {2021},
	keywords = {ATAI, ATAI scale, Artificial Intelligence, Attitude Towards Artificial Intelligence, Chinese, English, German},
	pages = {109--118},
}

@article{schepman_general_2023,
	title = {The {General} {Attitudes} towards {Artificial} {Intelligence} {Scale} ({GAAIS}): {Confirmatory} {Validation} and {Associations} with {Personality}, {Corporate} {Distrust}, and {General} {Trust}},
	volume = {39},
	issn = {1044-7318},
	shorttitle = {The {General} {Attitudes} towards {Artificial} {Intelligence} {Scale} ({GAAIS})},
	url = {https://doi.org/10.1080/10447318.2022.2085400},
	doi = {10.1080/10447318.2022.2085400},
	abstract = {Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public’s attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.},
	number = {13},
	urldate = {2025-10-21},
	journal = {International Journal of Human–Computer Interaction},
	author = {Schepman, Astrid and Rodway, Paul},
	month = aug,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2022.2085400},
	pages = {2724--2741},
}

@article{ibrahim_technology_2025,
	title = {The technology acceptance model and adopter type analysis in the context of artificial intelligence},
	volume = {7},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1496518/full},
	doi = {10.3389/frai.2024.1496518},
	abstract = {IntroductionArtificial Intelligence (AI) is a transformative technology impacting various sectors of society and the economy. Understanding the factors influencing AI adoption is critical for both research and practice. This study focuses on two key objectives: (1) validating an extended version of the Technology Acceptance Model (TAM) in the context of AI by integrating the Big Five personality traits and AI mindset, and (2) conducting an exploratory k-prototype analysis to classify AI adopters based on demographics, AI-related attitudes, and usage patterns.MethodsA sample of N = 1,007 individuals individuals (60\% female; M = 30.92; SD = 8.63 years) was collected. Psychometric data were obtained using validated scales for TAM constructs, Big Five personality traits, and AI mindset. Regression analysis was used to validate TAM, and a k-prototype clustering algorithm was applied to classify participants into adopter categories.ResultsThe psychometric analysis confirmed the validity of the extended TAM. Perceived usefulness was the strongest predictor of attitudes towards AI usage (β = 0.34, p {\textless} 0.001), followed by AI mindset scale growth (β = 0.28, p {\textless} 0.001). Additionally, openness was positively associated with perceived ease of use (β = 0.15, p {\textless} 0.001). The k-prototype analysis revealed four distinct adopter clusters, consistent with the diffusion of innovations model: early adopters (n = 218), early majority (n = 331), late majority (n = 293), and laggards (n = 165).DiscussionThe findings highlight the importance of perceived usefulness and AI mindset in shaping attitudes toward AI adoption. The clustering results provide a nuanced understanding of AI adopter types, aligning with established innovation diffusion theories. Implications for AI deployment strategies, policy-making, and future research directions are discussed.},
	language = {English},
	urldate = {2025-10-21},
	journal = {Frontiers in Artificial Intelligence},
	author = {Ibrahim, Fabio and Münscher, Johann-Christoph and Daseking, Monika and Telle, Nils-Torge},
	month = jan,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {AI mindset, artificial Intelligence, big five, early adopter, late adopter, technology acceptance model},
}

@article{baron_moderator-mediator_1986,
	title = {The moderator-mediator variable distinction in social psychological research: {Conceptual}, strategic, and statistical considerations},
	volume = {51},
	shorttitle = {The moderator-mediator variable distinction in social psychological research},
	doi = {10.1037//0022-3514.51.6.1173},
	abstract = {In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators. (46 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
	journal = {Journal of Personality and Social Psychology},
	author = {Baron, Reuben and Kenny, David},
	month = jan,
	year = {1986},
	pages = {1173--1182},
}

@incollection{nunnally_psychometric_1994,
	address = {New York},
	edition = {3rd},
	series = {{McGraw}-{Hill} series in psychology},
	title = {Psychometric {Theory}},
	copyright = {McGraw-Hill},
	isbn = {978-0-07-047849-7},
	url = {https://search.library.berkeley.edu/discovery/fulldisplay/alma991055658409706532/01UCS_BER:UCB},
	abstract = {Like the previous edition, this text is designed as a comprehensive text in measurement for researchers and for use in graduate courses in psychology, education and areas of business such as management and marketing. It is intended to consider the broad measurement problems that arise in these areas and is written for a reader who needs only a basic background in statistics to comprehend the material. It also combines classical procedures that explain variance with modern inferential procedures.},
	language = {en},
	booktitle = {Psychometric {Theory}},
	publisher = {McGraw-Hill},
	author = {Nunnally, Jum C. and Bernstein, Ira H.},
	year = {1994},
	pages = {752},
}

@misc{nunnally_psychometric_nodate,
	title = {Psychometric theory / {Jum} {C}. {Nunnally}, {Ira} {H}. {Bernstein}. - {University} of {California} {Berkeley}},
	url = {https://search.library.berkeley.edu/discovery/fulldisplay/alma991055658409706532/01UCS_BER:UCB},
	abstract = {(Publisher-supplied data) The classic text is Psychometric Theory. Like the previous edition, this text is designed as a comprehensive text in measurement for researchers and for use in graduate courses in psychology, education and areas of business such as management and marketing. It is intended to consider the broad measurement problems that arise in these areas and is written for a reader who needs only a basic background in statistics to comprehend the material. It also combines classical procedures that explain variance with modern inferential procedures.},
	language = {en},
	urldate = {2025-10-07},
	author = {Nunnally, Jum C. and Bernstein, Ira H.},
}

@incollection{gade_konfirmatorische_2020,
	title = {Konfirmatorische {Faktorenanalyse} ({CFA})},
	isbn = {978-3-662-61531-7},
	abstract = {Das Kapitel bietet eine Einführung in die Grundlagen der konfirmatorischen Faktorenanalyse (CFA). Im Rahmen der modernen Testkonstruktion stellt die CFA ein wichtiges Instrument zur Überprüfung der Dimensionalität und damit der faktoriellen Validität eines Tests dar. So können die theoretischen Annahmen eines Modells wie die Anzahl der Faktoren und die Zuordnung der Testitems zu den Faktoren explizit als Hypothesen aufgestellt und getestet werden. In diesem Kapitel werden theoretische Bezüge der CFA zur Klassischen Testtheorie (KTT) hergestellt, praktische Aspekte der Hypothesenbildung, Modellspezifikation und -identifikation behandelt sowie ein kurzer Überblick über Schätzverfahren und Gütekriterien zur Modellevaluation gegeben. Die CFA wird für ausgewählte ein- und mehrdimensionale Modelle an einem empirischen Beispiel vorgestellt. Der Einsatz der CFA zur Überprüfung der Messäquivalenz von Items, die für die Reliabilitätsschätzung von Bedeutung ist, werden ebenso besprochen wie Möglichkeiten des Modellvergleichs, der Modellmodifikation und der Überprüfung der Messinvarianz eines Tests über verschiedene Gruppen oder Messzeitpunkte hinweg.},
	author = {Gäde, Jana C. and Schermelleh-Engel, Karin and Brandt, Holger},
	month = aug,
	year = {2020},
	doi = {10.1007/978-3-662-61532-4_24},
	pages = {615--659},
}

@book{joreskog_lisrel_1981,
	title = {{LISREL} {V}: {Analysis} of linear structural relationships by maximum likelihood and least squares methods.},
	copyright = {Scientific Software},
	language = {en},
	publisher = {University of Uppsala},
	author = {Jöreskog, Karl G. and Sörbom, Dag},
	year = {1981},
}

@article{anderson_structural_1988,
	title = {Structural equation modeling in practice: {A} review and recommended two-step approach},
	volume = {103},
	issn = {1939-1455},
	shorttitle = {Structural equation modeling in practice},
	doi = {10.1037/0033-2909.103.3.411},
	abstract = {In this article, we provide guidance for substantive researchers on the use of structural equation modeling in practice for theory testing and development. We present a comprehensive, two-step modeling approach that employs a series of nested models and sequential chi-square difference tests. We discuss the comparative advantages of this approach over a one-step approach. Considerations in specification, assessment of fit, and respecification of measurement models using confirmatory factor analysis are reviewed. As background to the two-step approach, the distinction between exploratory and confirmatory analysis, the distinction between complementary approaches for theory testing versus predictive application, and some developments in estimation methods also are discussed. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Anderson, James C. and Gerbing, David W.},
	year = {1988},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Confirmatory Factor Analysis, Factor Analysis, Mathematical Modeling, Statistical Analysis},
	pages = {411--423},
}

@techreport{bentler_eqs_1995,
	address = {Enico, California CA},
	type = {Manual},
	title = {{EQS} 6 {Structural} equations program manual},
	url = {https://www3.nd.edu/~kyuan/courses/sem/EQS-Manual6.pdf},
	language = {en},
	number = {6},
	urldate = {2025-07-10},
	institution = {Multivariate Software, Inc.},
	author = {Bentler, Peter M.},
	year = {1995},
	pages = {418},
}

@article{hu_cutoff_1999,
	title = {Cutoff criteria for fit indexes in covariance structure analysis: {Conventional} criteria versus new alternatives},
	volume = {6},
	issn = {1532-8007},
	shorttitle = {Cutoff criteria for fit indexes in covariance structure analysis},
	doi = {10.1080/10705519909540118},
	abstract = {Examines the adequacy of the "rules of thumb" conventional cutoff criteria and several new alternatives for various fit indexes used to evaluate model fit in practice. Using a 2-index presentation strategy, which includes using the maximum likelihood (ML)-based standardized root mean squared residual (SRMR) and supplementing it with either Tucker-Lewis Index (TLI), Bollen's (1989) Fit Index (BL89), Relative Noncentrality Index (RNI), Comparative Fit Index (CFI), Gamma Hat, McDonald's Centrality Index (Mc), or root mean squared error of approximation (RMSEA), various combinations of cutoff values from selected ranges of cutoff criteria for the ML-based SRMR and a given supplemental fit index were used to calculate rejection rates for various types of true-population and misspecified models, models with misspecified factor covariance and models with misspecified factor loading. The results suggest that, for the ML method, a cutoff value close to .95 for TLI, BL89, CFI, RNI, and Gamma Hat; a value close to .90 for Mc; a cutoff value close to .08 for SRMR; and one close to .06 for RMSEA are needed before it can be concluded that there is a good fit between the hypothesized model and the observed data. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Structural Equation Modeling},
	author = {Hu, Li-tze and Bentler, Peter M.},
	year = {1999},
	note = {Place: US
Publisher: Lawrence Erlbaum},
	keywords = {Factor Analysis, Goodness of Fit, Maximum Likelihood},
	pages = {1--55},
}

@article{browne_alternative_1992,
	title = {Alternative {Ways} of {Assessing} {Model} {Fit}},
	volume = {21},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124192021002005},
	doi = {10.1177/0049124192021002005},
	abstract = {This article is concerned with measures of fit of a model. Two types of error involved in fitting a model are considered. The first is error of approximation which involves the fit of the model, with optimally chosen but unknown parameter values, to the population covariance matrix. The second is overall error which involves the fit of the model, with parameter values estimated from the sample, to the population covariance matrix. Measures of the two types of error are proposed and point and interval estimates of the measures are suggested. These measures take the number of parameters in the model into account in order to avoid penalizing parsimonious models. Practical difficulties associated with the usual tests of exact fit or a model are discussed and a test of “close fit” of a model is suggested.},
	language = {EN},
	number = {2},
	urldate = {2025-10-07},
	journal = {Sociological Methods \& Research},
	author = {Browne, Michael W. and Cudeck, Robert},
	month = nov,
	year = {1992},
	note = {Publisher: SAGE Publications Inc},
	pages = {230--258},
}

@article{steiger_statistically-based_1980,
	title = {Statistically-based tests for the number of factors ({Handout})},
	volume = {23},
	issn = {1532-8007},
	doi = {10.1080/10705511.2016.1217487},
	abstract = {This note presents the original Steiger-Lind (1980) handout entitled, “Statistically-Based Tests for the Number of Common Factors”, distributed to all in attendance at the talk given at the annual meeting of the Psychometric Society in Iowa City, Iowa. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {6},
	journal = {Structural Equation Modeling},
	author = {Steiger, James H. and Lind, J.C.},
	year = {1980},
	note = {Place: United Kingdom
Publisher: Taylor \& Francis},
	keywords = {Models, Psychometrics, Simulation, Statistics},
	pages = {777--781},
}

@article{tucker_reliability_1973,
	title = {A reliability coefficient for maximum likelihood factor analysis},
	volume = {38},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02291170},
	doi = {10.1007/BF02291170},
	abstract = {Maximum likelihood factor analysis provides an effective method for estimation of factor matrices and a useful test statistic in the likelihood ratio for rejection of overly simple factor models. A reliability coefficient is proposed to indicate quality of representation of interrelations among attributes in a battery by a maximum likelihood factor analysis. Usually, for a large sample of individuals or objects, the likelihood ratio statistic could indicate that an otherwise acceptable factor model does not exactly represent the interrelations among the attributes for a population. The reliability coefficient could indicate a very close representation in this case and be a better indication as to whether to accept or reject the factor solution.},
	language = {en},
	number = {1},
	urldate = {2025-10-07},
	journal = {Psychometrika},
	author = {Tucker, Ledyard R. and Lewis, Charles},
	month = mar,
	year = {1973},
	keywords = {Common Factor, Factor Model, Factor Solution, Likelihood Ratio Statistic, Reliability Coefficient},
	pages = {1--10},
}

@article{bentler_comparative_1990,
	title = {Comparative fit indexes in structural models},
	volume = {107},
	issn = {1939-1455},
	url = {https://psycnet.apa.org/record/1990-13755-001},
	doi = {10.1037/0033-2909.107.2.238},
	abstract = {Normed and nonnormed fit indexes are frequently used as adjuncts to chi-square statistics for evaluating the fit of a structural model. A drawback of existing indexes is that they estimate no known population parameters. A new coefficient is proposed to summarize the relative reduction in the noncentrality parameters of 2 nested models. Two estimators of the coefficient yield new normed (CFIN) and nonnormed (FIN) fit indexes. CFIN avoids the underestimation of fit often noted in small samples for P. M. Bentler and D. G. Bonett's (see record 1981-06898-001) normed fit index (NFIN). FIN is a linear function of Bentler and Bonett's nonnormed fit index (NNFIN) that avoids the extreme underestimation and overestimation often found in NNFIN. Asymptotically, CFIN, FIN, NFIN, and a new index developed by K. A. Bollen (1989) are equivalent measures of comparative fit, whereas NNFIN measures relative fit by comparing noncentrality per degree of freedom. All of the indexes are generalized to permit use of Wald and Lagrange multiplier statistics. An example illustrates the behavior of these indexes under conditions of correct specification and misspecification. The new fit indexes perform very well at all sample sizes. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Bulletin},
	author = {Bentler, P. M.},
	year = {1990},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Goodness of Fit, Methodology, Models, Population (Statistics), Statistical Estimation, Statistical Norms},
	pages = {238--246},
}

@article{kano_software_1997,
	title = {Software},
	volume = {24},
	issn = {1349-6964},
	url = {https://doi.org/10.2333/bhmk.24.85},
	doi = {10.2333/bhmk.24.85},
	abstract = {Eleven CAS program authors were invited to write a description of their own program and answer the questionnaire enclosed. We received seven responses. Following their descriptions, the answers to the questionnaire are summarized in tables. Additional information on CSA programs is found in Waller (1993, Applied Psychological Measurement, Vol. 17, pp. 73-100) and Howell (1996, Journal of Marketing Research, Vol. 33, pp. 377-381).},
	language = {en},
	number = {1},
	urldate = {2025-10-07},
	journal = {Behaviormetrika},
	author = {Kano, Yutaka and Arbuckle, James L. and McDonald, Roderick P. and Fraser, Colin and Bentier, Peter M. and Jöreskog, Karl G. and Arminger, G. and Browne, Michael W. and Steiger, James H.},
	month = jan,
	year = {1997},
	pages = {85--125},
}

@incollection{tversky_framing_1986,
	series = {Logic, {Methodology} and {Philosophy} of {Science} {VII}},
	title = {The {Framing} of {Decisions} and the {Evaluation} of {Prospects}},
	volume = {114},
	url = {https://www.sciencedirect.com/science/article/pii/S0049237X09707104},
	abstract = {The chapter presents a series of demonstrations in which seemingly inconsequential changes in the formulation of choice problems caused significant shifts of preference. The inconsistencies were traced to the interaction of two sets of factors: variation in the framing of acts, contingencies and outcomes, and the characteristic non-linearities of values and decision weights. The demonstrated effects are large and systematic, although by no means universal. They occur when the outcomes concern the loss of human lives as well as in choices about money; they are not restricted to hypothetical questions and are not eliminated by monetary incentives. The chapter is concerned primarily with the descriptive question of how decisions are made and emphasizes that the psychology of choice is also relevant to the normative question of how decisions ought to be made. The framing of acts and outcomes can also reflect the acceptance or rejection of responsibility for particular consequences, and the deliberate manipulation of framing is commonly used as an instrument of self-control. When framing influences the experience of consequences, the adoption of a decision frame is an ethically significant act.},
	language = {en},
	urldate = {2025-09-29},
	booktitle = {Studies in {Logic} and the {Foundations} of {Mathematics}},
	publisher = {Elsevier},
	author = {Tversky, Amos and Kahneman, Daniel},
	editor = {Barcan Marcus, Ruth and Dorn, Georg J. W. and Weingartner, Paul},
	month = jan,
	year = {1986},
	doi = {10.1016/S0049-237X(09)70710-4},
	pages = {503--520},
}

@article{freling_when_2014,
	title = {When not to accentuate the positive: {Re}-examining valence effects in attribute framing},
	volume = {124},
	issn = {0749-5978},
	shorttitle = {When \textit{not} to accentuate the positive},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597814000090},
	doi = {10.1016/j.obhdp.2013.12.007},
	abstract = {While the expanding body of attribute framing literature provides keen insights into individual judgments and evaluations, a lack of theoretical perspective inhibits scholars from more fully extending research foci beyond a relatively straightforward examination of message content. The current research applies construal level theory to attribute framing research. The authors conduct a meta-analysis of 107 published articles and then conceptually expand this knowledge base by synthesizing attribute framing research and construal level concepts. Results suggest that attribute framing is most effective when there is congruence between the construal level evoked in a frame and the evaluator’s psychological distance from the framed event. A follow-up experiment confirms that the congruence between a frame’s construal level and psychological distance—not simply its valence—appears to be driving attribute framing effects. This research proposes to shift the focus in attribute framing research from that of message composition to a more complex relationship between the message and the recipient.},
	number = {2},
	urldate = {2025-09-28},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Freling, Traci H. and Vincent, Leslie H. and Henard, David H.},
	month = jul,
	year = {2014},
	keywords = {Construal level theory, Framing effects, Message framing, Meta-analysis},
	pages = {95--109},
}

@article{cugurullo_fear_2024,
	title = {Fear of {AI}: an inquiry into the adoption of autonomous cars in spite of fear, and a theoretical framework for the study of artificial intelligence technology acceptance},
	volume = {39},
	issn = {1435-5655},
	shorttitle = {Fear of {AI}},
	url = {https://doi.org/10.1007/s00146-022-01598-6},
	doi = {10.1007/s00146-022-01598-6},
	abstract = {Artificial intelligence (AI) is becoming part of the everyday. During this transition, people’s intention to use AI technologies is still unclear and emotions such as fear are influencing it. In this paper, we focus on autonomous cars to first verify empirically the extent to which people fear AI and then examine the impact that fear has on their intention to use AI-driven vehicles. Our research is based on a systematic survey and it reveals that while individuals are largely afraid of cars that are driven by AI, they are nonetheless willing to adopt this technology as soon as possible. To explain this tension, we extend our analysis beyond just fear and show that people also believe that AI-driven cars will generate many individual, urban and global benefits. Subsequently, we employ our empirical findings as the foundations of a theoretical framework meant to illustrate the main factors that people ponder when they consider the use of AI tech. In addition to offering a comprehensive theoretical framework for the study of AI technology acceptance, this paper provides a nuanced understanding of the tension that exists between the fear and adoption of AI, capturing what exactly people fear and intend to do.},
	language = {en},
	number = {4},
	urldate = {2025-09-28},
	journal = {AI \& SOCIETY},
	author = {Cugurullo, Federico and Acheampong, Ransford A.},
	month = aug,
	year = {2024},
	keywords = {Artificial intelligence, Autonomous cars, Fear, Technology acceptance, Theoretical framework, Urban artificial intelligences},
	pages = {1569--1584},
}

@misc{cunningham_how_2025,
	address = {Cambridge, MA 02138},
	title = {How {People} {Use} {ChatGPT}},
	copyright = {National Bureau of Economic Research, 1050 Massachusetts Avenue, Cambridge, MA 02138},
	shorttitle = {How {People} {Use} {ChatGPT}},
	url = {https://www.nber.org/papers/w34255},
	doi = {10.3386/w34255},
	language = {en},
	urldate = {2025-09-28},
	author = {Cunningham, Thomas and Deming, J. David and Hitzig, Zoe and Ong, Christopher and Yan Shan, Carl and Wadman, Kevin},
	month = sep,
	year = {2025},
}

@article{baroni_ai-tam_2022,
	title = {{AI}-{TAM}: a model to investigate user acceptance and collaborative intention in human-in-the-loop {AI} applications},
	volume = {9},
	copyright = {Copyright (c) 2022 Ilaria Baroni, Gloria Re Calegari, Damiano Scandolari, Irene Celino},
	issn = {2330-8001},
	shorttitle = {{AI}-{TAM}},
	url = {https://hcjournal.org/index.php/jhc/article/view/134},
	doi = {10.15346/hc.v9i1.134},
	abstract = {More and more frequently, digital applications make use of Artificial Intelligence (AI) capabilities to provide advanced features; on the other hand, human-in-the-loop approaches are on the rise to involve people in AI-powered pipelines for data collection, results validation and decision-making.
Does the introduction of AI features affect user acceptance? Does the AI result quality affect people willingness to use such applications? Does the additional user effort required in human-in-the-loop mechanisms change the application adoption and use?
This study aims to provide a reference approach to answer those questions. We propose a model that extends the Technology Acceptance Model (TAM) with further constructs explicitly related to AI (user trust in AI and perceived quality of AI output, from XAI literature) and collaborative intention (willingness to contribute to AI pipelines).
We tested the proposed model with an application for car damage claim reporting with AI-powered damage estimation for insurance customers. The results showed that the XAI related factors have a strong and positive effect on the behavioural intention, the perceived usefulness and the ease of use of the application. Moreover, there is a strong link between the behavioural intention and the collaborative intention, indicating that indeed human-in-the-loop approaches can be successfully adopted in final user applications.},
	language = {en},
	number = {1},
	urldate = {2025-09-28},
	journal = {Human Computation},
	author = {Baroni, Ilaria and Calegari, Gloria Re and Scandolari, Damiano and Celino, Irene},
	month = may,
	year = {2022},
	pages = {1--21},
}

@article{ray_can_2019,
	title = {Can {You} {Explain} {That}? {Lucid} {Explanations} {Help} {Human}-{AI} {Collaborative} {Image} {Retrieval}},
	volume = {7},
	shorttitle = {Can {You} {Explain} {That}?},
	doi = {10.1609/hcomp.v7i1.5275},
	abstract = {While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as “helpful” significantly improve human performance, “incorrect” and “unhelpful” explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanation on a human-AI collaborative task.},
	journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
	author = {Ray, Arijit and Yao, Yi and Kumar, Rakesh and Divakaran, Ajay and Burachas, Giedrius},
	month = oct,
	year = {2019},
	pages = {153--161},
}

@article{lee_trust_2004,
	title = {Trust in {Automation}: {Designing} for {Appropriate} {Reliance}},
	volume = {46},
	issn = {0018-7208},
	shorttitle = {Trust in {Automation}},
	url = {https://journals.sagepub.com/action/showAbstract},
	doi = {10.1518/hfes.46.1.50_30392},
	abstract = {Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.},
	language = {EN},
	number = {1},
	urldate = {2025-09-23},
	journal = {Human Factors},
	author = {Lee, John D. and See, Katrina A.},
	month = mar,
	year = {2004},
	note = {Publisher: SAGE Publications Inc},
	pages = {50--80},
}

@incollection{sasmannshausen_vertrauen_2020,
	address = {Wiesbaden},
	title = {Vertrauen in {KI} – {Eine} empirische {Analyse} innerhalb des {Produktionsmanagements}},
	isbn = {978-3-658-29550-9},
	url = {https://doi.org/10.1007/978-3-658-29550-9_10},
	abstract = {Ob Industrie 4.0, Big Data, Predictive Analytics oder Robotik – die digitale Transformation hat viele Facetten. Sie führt aber nicht nur zu einem Paradigmenwechsel in der industriellen Produktion. Auch komplexe kognitive Tätigkeiten sind durch die fortschreitende Entwicklung der künstlichen Intelligenz (KI) einem Wandel unterzogen. Smarte Assistenten halten Einzug in die Arbeitswelt und erfordern eine Kooperation von KI und Mensch. KI agiert anders als bisherige Systeme – autonom statt automatisch – und somit bisweilen für den Menschen unerwartet, überraschend und nicht immer nachvollziehbar. In dieser Konstellation ist Vertrauen ein essenzieller Faktor, der über das Funktionieren der Mensch-KI-Kooperation entscheidet. Im Rahmen des vorliegenden Beitrages sollen daher mit einer empirischen Analyse innerhalb des Produktionsmanagements die Einflussfaktoren auf das Vertrauen sowie deren Wirkmechanismen identifiziert werden.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Künstliche {Intelligenz} in {Wirtschaft} \& {Gesellschaft}: {Auswirkungen}, {Herausforderungen} \& {Handlungsempfehlungen}},
	publisher = {Springer Fachmedien},
	author = {Saßmannshausen, Till Moritz and Heupel, Thomas},
	editor = {Buchkremer, Rüdiger and Heupel, Thomas and Koch, Oliver},
	year = {2020},
	doi = {10.1007/978-3-658-29550-9_10},
	pages = {169--192},
}

@misc{singh_generative_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Generative {AI} through the {Lens} of {Technology} {Acceptance} {Model}},
	url = {https://papers.ssrn.com/abstract=4953174},
	doi = {10.2139/ssrn.4953174},
	abstract = {This paper applies the Technology Acceptance Model (TAM) to explore the adoption of Generative AI systems like ChatGPT, Claude, Gemini, and Copilot. It analyzes how Perceived Usefulness (PU) and Perceived Ease of Use (PEOU) influence the increasing adoption of these technologies. The study finds that simplifying technical complexity enhances users' perceptions of utility and ease, boosting adoption intentions. Key factors include effective organizational training, intuitive design, and strategic partnerships. The paper also addresses challenges such as trust and ethical concerns, offering insights into user acceptance in the rapidly evolving AI landscape.},
	language = {en},
	urldate = {2025-09-23},
	publisher = {Social Science Research Network},
	author = {Singh, Dr Preet Deep},
	month = sep,
	year = {2024},
	keywords = {Dr Preet Deep Singh, Generative AI through the Lens of Technology Acceptance Model, SSRN},
}

@article{kelly_what_2023,
	title = {What factors contribute to the acceptance of artificial intelligence? {A} systematic review},
	volume = {77},
	issn = {0736-5853},
	shorttitle = {What factors contribute to the acceptance of artificial intelligence?},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585322001587},
	doi = {10.1016/j.tele.2022.101925},
	abstract = {Artificial Intelligence (AI) agents are predicted to infiltrate most industries within the next decade, creating a personal, industrial, and social shift towards the new technology. As a result, there has been a surge of interest and research towards user acceptance of AI technology in recent years. However, the existing research appears dispersed and lacks systematic synthesis, limiting our understanding of user acceptance of AI technologies. To address this gap in the literature, we conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and meta-Analysis guidelines using five databases: EBSCO host, Embase, Inspec (Engineering Village host), Scopus, and Web of Science. Papers were required to focus on both user acceptance and AI technology. Acceptance was defined as the behavioural intention or willingness to use, buy, or try a good or service. A total of 7912 articles were identified in the database search. Sixty articles were included in the review. Most studies (n = 31) did not define AI in their papers, and 38 studies did not define AI for their participants. The extended Technology Acceptance Model (TAM) was the most frequently used theory to assess user acceptance of AI technologies. Perceived usefulness, performance expectancy, attitudes, trust, and effort expectancy significantly and positively predicted behavioural intention, willingness, and use behaviour of AI across multiple industries. However, in some cultural scenarios, it appears that the need for human contact cannot be replicated or replaced by AI, no matter the perceived usefulness or perceived ease of use. Given that most of the methodological approaches present in the literature have relied on self-reported data, further research using naturalistic methods is needed to validate the theoretical model/s that best predict the adoption of AI technologies.},
	urldate = {2025-09-23},
	journal = {Telematics and Informatics},
	author = {Kelly, Sage and Kaye, Sherrie-Anne and Oviedo-Trespalacios, Oscar},
	month = feb,
	year = {2023},
	keywords = {AI, Human factors, Machine learning, Psychosocial models, Social robotics, User acceptance},
	pages = {101925},
}

@article{budnik_can_2025,
	title = {Can {We} {Trust} {Artificial} {Intelligence}?},
	volume = {38},
	doi = {10.1007/s13347-024-00820-1},
	abstract = {In view of the dramatic advancements in the development of artificial intelligence technology in recent years, it has become a commonplace to demand that AI systems be trustworthy. This view presupposes that it is possible to trust AI technology in the first place. The aim of this paper is to challenge this view. In order to do that, it is argued that the philosophy of trust really revolves around the problem of how to square the epistemic and the normative dimensions of trust. Given this double nature of trust it is possible to extract a threefold challenge to the defenders of the possibility of AI trust without presupposing any particular trust theory. They have to show (1) how trust in AI systems is more than mere reliance; (2) how AI systems can become objects of normative expectations; and (3) how the resulting attitude gives human agents reassurance in their interactions with AI systems. In order to demonstrate how difficult this task is, the threefold challenge is then applied to two recent accounts that defend the possibility of trust in AI systems. By way of conclusion it is suggested that instead of trusting AI systems, we should strive to make them reliable.},
	journal = {Philosophy \& Technology},
	author = {Budnik, Christian},
	month = jan,
	year = {2025},
}

@article{kim_communicating_2022,
	title = {Communicating the {Limitations} of {AI}: {The} {Effect} of {Message} {Framing} and {Ownership} on {Trust} in {Artificial} {Intelligence}},
	volume = {39},
	shorttitle = {Communicating the {Limitations} of {AI}},
	doi = {10.1080/10447318.2022.2049134},
	abstract = {Trust plays an essential role in the interaction between humans and artificial intelligence (AI). To promote trust in AI, information about the AI’s performance should be communicated well to the users. Accordingly, this paper investigates how information about AI performance should be presented, focusing on message framing and the ownership of decisions. A 2 (ownership: no ownership vs. ownership) × 3 (message framing: no information vs. negative information vs. positive information) between-subjects experiment was conducted (N = 120). Participants were asked to choose items to help them survive in the desert, supported by an AI decision. The results showed that participants without decision ownership perceived higher trust than those with decision ownership. Also, trust was perceived to be higher when participants were not given performance information than when they were. The results indicate the importance of carefully communicating with AI. The implications of this study are discussed.},
	journal = {International Journal of Human-Computer Interaction},
	author = {Kim, Taenyun and Song, Hayeon},
	month = apr,
	year = {2022},
	pages = {1--11},
}

@techreport{schreiber_psychologie_2020,
	title = {Psychologie und künstliche {Intelligenz} ({KI}) : {Parallelen}, {Chancen}, {Herausforderungen} und ein {Blick} in die nahe {Zukunft}},
	shorttitle = {Psychologie und künstliche {Intelligenz} ({KI})},
	url = {https://digitalcollection.zhaw.ch/handle/11475/19724},
	abstract = {Im vorliegenden Kapitel wird die KI im Kontext der Psychologie, der Lehre des menschlichen Verhaltens und Erlebens, betrachtet. Dabei wird mit der Theorie der Persönlichkeits-System-Interaktionen (PSI-Theorie) (Kuhl 2001) eine psychologische Theorie als Basis genommen, die von zwei Arten der Intelligenz ausgeht, nämlich einer analytischen und einer intuitiven. Die PSI-Theorie eignet sich als Grundlage für eine Reflexion der Chancen und Herausforderungen im Zusammenhang mit der KI, weil sich dabei die konkrete Frage stellt, ob KI „nur“ die analytische Intelligenz abdecken oder ob sie auch intuitiv intelligent sein und dadurch menschliche Züge annehmen kann. Nach einer Auslegeordnung auf der Basis der PSI-Theorie wird ein spezieller Fokus auf die psychologische Intelligenzforschung gelegt. Dabei wird aufgezeigt, dass sich Theorie und Praxis aufgrund der unterschiedlichen erkenntnistheoretischen Schwerpunkte in den vergangenen Jahren auseinanderentwickelt haben und eine neue Praxis- und Forschungsagenda postuliert. Mit Bezug zur Intelligenzforschung werden sodann Gedanken zur Entwicklung der KI in der nahen Zukunft formuliert. Das Kapitel wird abgerundet durch die Beschreibung eines konkreten praxisbezogenen Projektes mit starkem Explorationscharakter, welches zum Ziel hat, KI und psychologische Beratungspraxis in den Bereichen Emotions- und Spracherkennung zu verbinden.},
	language = {de},
	urldate = {2025-09-23},
	institution = {Springer},
	author = {Schreiber, Marc and Gloor, Peter A.},
	month = feb,
	year = {2020},
	doi = {10.1007/978-3-662-60465-6_12},
	note = {ISBN: 9783662604649
Publication Title: Angewandte Psychologie in der Arbeitswelt},
	pages = {161--180},
}

@article{karg_ai_2024,
	title = {In {AI} {We} {Trust}. {Aspekte} des {Vertrauens} in {ChatGPT}},
	url = {https://irf.fhnw.ch/handle/11654/47513},
	abstract = {Die Relevanz von Künstlicher Intelligenz (KI) nimmt zu, ebenso wie die diesbezüglichen Sicherheitsbedenken. Dies führt zur Forderung nach einer vertrauenswürdigeren KI, welcher sich Forschende zunehmend annehmen. Es existiert jedoch keine allgemeine Theorie zu Vertrauen in KI; so dient häufig das Modell Trust in Automation von Lee und See (2004) als Forschungsgrundlage, so kann Automation als Basis von KI betrachtet werden. Gleichzeitig wird jedoch in Frage gestellt, ob dieses Modell auf KI übertragen werden kann. Entsprechend wird in dieser Arbeit im Kontext von ChatGPT untersucht, welche Faktoren das Vertrauen in KI beeinflussen und wie sich dies auf die Nutzungsintention auswirkt. Basierend auf Trust in Automation, ergänzt durch die Variable Nutzungsintention, wurde hierzu ein Pfadmodell abgeleitet. Zudem wurden Einflussfaktoren der Neigung zum Vertrauen sowie die zeitliche Veränderung des Zusammenhangs des Vertrauens im Kontext einer Intervention untersucht. Die Datenerhebung erfolgte anhand validierter Fragebögen. Im Rahmen der Datenanalyse wurden die Daten von insgesamt 105 Studierenden berücksichtigt, während für die ergänzende Längsschnittanalyse 10 Datensätze herangezogen wurden. Die Resultate bestätigen das konzeptuelle Pfadmodell nicht, was eine Respezifikation erforderlich machte. Dies hat dazu geführt, dass zusätzliche Pfade von der Neigung zum Vertrauen zu den drei Faktoren der Vertrauenswürdigkeit identifiziert wurden. Das Modell zeigt mit einer Ausnahme die erwarteten positiven Einflüsse. Dabei ist der Einfluss des Vertrauens auf die Nutzungsintention geringer als erwartet. Die Längsschnittanalyse hat keinerlei Veränderungen zwischen den beiden Messzeitpunkten offenbart. Die Ergebnisse unterstützen grösstenteils die Erwartungen, stellen jedoch auch bisherige Annahmen in Frage. So wird Vertrauen möglicherweise eine grössere Rolle zugeschrieben als tatsächlich vorliegt.},
	language = {de},
	urldate = {2025-09-23},
	author = {Karg, Jona},
	month = sep,
	year = {2024},
	note = {Publisher: Hochschule für Angewandte Psychologie FHNW},
}

@article{lelli_co-intelligence_2025,
	title = {Co-intelligence in design: the importance of trust in artificial intelligence},
	volume = {5},
	shorttitle = {Co-intelligence in design},
	doi = {10.1017/pds.2025.10111},
	abstract = {As Generative Artificial Intelligence (GenAI) gets integrated in design processes, building trust in these systems is critical for effective human-AI collaboration. This study introduces a framework aimed at translating the abstract concept of trust into practical strategies for design teams, focusing on four trust factors: transparency, accountability, similarity, and performance. We tested the framework’s impact on trust-building and trust learning using a mixed-methods approach, incorporating design tasks and structured workshops involving university students. The results highlight the framework’s ability to enhance participants’ understanding of trust in AI. Insights from this study contribute to advancing educational approaches for embedding trust in AI-driven design, revealing that design activities alone are not enough to impact trust learning.},
	journal = {Proceedings of the Design Society},
	author = {Lelli, Chiara and Chiarello, Filippo and Giordano, Vito},
	month = aug,
	year = {2025},
	pages = {971--980},
}

@book{hoffmann_psychologie_2025,
	address = {Berlin, Heidelberg},
	title = {Die {Psychologie} und die {Künstliche} {Intelligenz}: {Maschinen}, {Bewusstsein} und die menschliche {Psyche}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-70965-8 978-3-662-70966-5},
	shorttitle = {Die {Psychologie} und die {Künstliche} {Intelligenz}},
	url = {https://link.springer.com/10.1007/978-3-662-70966-5},
	language = {de},
	urldate = {2025-09-23},
	publisher = {Springer},
	author = {Hoffmann, Oliver},
	year = {2025},
	doi = {10.1007/978-3-662-70966-5},
	keywords = {Bewusstsein und Emotionen, Denksysteme im Wandel, Emotionen und KI, Ethik und Moral der KI, Ethik und Verantwortung, KI als Bedrohung, KI im Alltag, Künstliche Intelligenz, Mensch und Maschine, Mensch versus Maschine, Psychologie des Vertrauens, Psychologie mit und durch KI, Psychologische Mechanismen, Selbstbild und Identität, Zukunft der Psychologie, maschinelles Lernen, menschliches Denken},
}

@book{kopp_vertrauen_2022,
	title = {Vertrauen in {Roboter} und dessen {Beeinflussbarkeit} durch sprachliches {Framing}: {Eine} empirische {Untersuchung} der {Interaktion} mit {Cobots} am {Arbeitsplatz}},
	isbn = {978-3-7315-1206-6},
	shorttitle = {Vertrauen in {Roboter} und dessen {Beeinflussbarkeit} durch sprachliches {Framing}},
	url = {https://library.oapen.org/handle/20.500.12657/60477},
	abstract = {Collaborative robots (cobots) enable human-robot interactions in the workplace without safety fences. An appropriate level of trust by employees is critical to the success of these interactions. Anthropomorphic perceptions and fears of technological replacement affect trust formation. They can be influenced by linguistic framing, as this interdisciplinary empirical study shows.},
	language = {German},
	urldate = {2025-09-23},
	publisher = {KIT Scientific Publishing},
	author = {Kopp, Tobias},
	year = {2022},
	doi = {10.5445/KSP/1000146827},
	note = {Accepted: 2023-01-03T12:44:05Z},
	keywords = {Anthropomorphism, Anthropomorphismus, Collaborative robots, Human-robot interaction, Kollaborierende Roboter, Linguistic framing, Mensch-Roboter-Interaktion, Sprachliches Framing, Trust, Vertrauen, thema EDItEUR::N History and Archaeology::NH History},
}

@incollection{rotzel_kunstliche_2024,
	address = {Wiesbaden},
	title = {Künstliche {Intelligenz} ({KI}) – unser bester {Freund}?},
	isbn = {978-3-658-43816-6},
	url = {https://doi.org/10.1007/978-3-658-43816-6_2},
	abstract = {Künstliche Intelligenz (KI) hat sich zu einer transformativen Kraft entwickelt, die verschiedene Aspekte der täglichen Arbeit beeinflusst. Es stellt sich die Frage: Können Menschen freundschaftliche Beziehungen zu KI-Entscheidungsunterstützungssystemen aufbauen oder werden diese Systeme nur als Werkzeuge betrachtet? In diesem Kapitel werden die Dynamik, die Herausforderungen und die Möglichkeiten von Mensch-KI-Interaktionen (MKI) untersucht, wobei ein besonderer Fokus auf die entscheidende Rolle des Vertrauens in dieser Interaktion gelegt wird. Das Vertrauen in KI wird durch kognitive, emotionale und soziale Faktoren beeinflusst. Zu den kognitiven Faktoren gehören die Transparenz und Interpretierbarkeit von KI-Systemen, zu den emotionalen Faktoren gehören die emotionale Bindung und das Verhältnis zwischen Menschen und KI-Agenten und zu den sozialen Faktoren gehören gesellschaftliche Normen und kulturelle Einflüsse. Das Spannungsverhältnis zwischen Automatisierungs- und Algorithmusvermeidungstendenzen stellt eine komplexe Herausforderung für MKI dar. Automatisierungsbias bedeutet, sich unhinterfragt auf KI-Empfehlungen zu verlassen. Die Tendenz zur Algorithmusvermeidung beschreibt die Ablehnung oder das Übergehen von KI-Empfehlungen zugunsten eines menschlichen Urteils. Um dieses Spannungsfeld zu bewältigen, müssen transparente und erklärbare KI-Systeme entwickelt und eine effektive Zusammenarbeit zwischen Menschen und KI gefördert werden. Durch die Berücksichtigung dieser Faktoren und die Stärkung des Vertrauens kann MKI zu einer informierteren Entscheidungsfindung und einer effektiven Nutzung der KI-Funktionen führen.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Vertrauen in {Künstliche} {Intelligenz}: {Eine} multi-perspektivische {Betrachtung}},
	publisher = {Springer Fachmedien},
	author = {Rötzel, Peter Gordon},
	editor = {Schork, Sabrina},
	year = {2024},
	doi = {10.1007/978-3-658-43816-6_2},
	keywords = {Entscheidungsempfehlung, Entscheidungshilfesysteme, Künstliche Intelligenz, Vertrauen},
	pages = {19--33},
}

@book{schork_vertrauen_2024,
	address = {Wiesbaden},
	title = {Vertrauen in {Künstliche} {Intelligenz}: {Eine} multi-perspektivische {Betrachtung}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-658-43815-9 978-3-658-43816-6},
	shorttitle = {Vertrauen in {Künstliche} {Intelligenz}},
	url = {https://link.springer.com/10.1007/978-3-658-43816-6},
	language = {de},
	urldate = {2025-09-23},
	publisher = {Springer Fachmedien},
	editor = {Schork, Sabrina},
	year = {2024},
	doi = {10.1007/978-3-658-43816-6},
	keywords = {Anwendungsnahe Forschung, ChatGPT, Interdisziplinarität, Künstliche Intelligenz, Menschenzentrierte Künstliche Intelligenz, Vertrauen},
}

@incollection{weitz_vertrauen_2021,
	address = {Wiesbaden},
	title = {Vertrauen und {Vertrauenswürdigkeit} bei sozialen {Robotern}},
	isbn = {978-3-658-31114-8},
	url = {https://doi.org/10.1007/978-3-658-31114-8_16},
	abstract = {Dieses Kapitel befasst sich mit der Vertrauensbeziehung zwischen Menschen und sozialen Robotern, stellt doch Vertrauen einen wichtigen Bestandteil für die Akzeptanz sozialer Roboter dar. Ausgehend von den Merkmalen sozialer Interaktionen zwischen Mensch und Roboter wird ein Überblick über verschiedene Definitionen von Vertrauen in diesem Kontext gegeben. Zudem werden theoretische Vertrauensmodelle und praktische Möglichkeiten der Erfassung von Vertrauen skizziert sowie der Vertrauensverlust als Folge von Roboterfehlern betrachtet. Es wird beleuchtet, wie Erklärbare Künstliche Intelligenz helfen kann, eine transparente Interaktion zwischen Roboter und Mensch zu ermöglichen und dadurch das Vertrauen in soziale Roboter (wieder-)herzustellen. Insbesondere wird auf die Gestaltungsmöglichkeiten und Herausforderungen beim Einsatz von Erklärungen im Bereich der Robotik eingegangen. Die Wirkung, die Erklärungen von Robotern auf die mentalen Modelle von Nutzer:innen haben, bildet den Abschluss dieses Kapitels.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Soziale {Roboter}: {Technikwissenschaftliche}, wirtschaftswissenschaftliche, philosophische, psychologische und soziologische {Grundlagen}},
	publisher = {Springer Fachmedien},
	author = {Weitz, Katharina},
	editor = {Bendel, Oliver},
	year = {2021},
	doi = {10.1007/978-3-658-31114-8_16},
	pages = {309--323},
}

@incollection{tschopp_vertrauen_2022,
	address = {Berlin, Heidelberg},
	title = {Vertrauen {Sie} {KI}? {Einblicke} in das {Thema} {Künstliche} {Intelligenz} und warum {Vertrauen} eine {Schlüsselrolle} im {Umgang} mit neuen {Technologien} spielt},
	isbn = {978-3-662-63117-1},
	shorttitle = {Vertrauen {Sie} {KI}?},
	url = {https://doi.org/10.1007/978-3-662-63117-1_16},
	abstract = {No trust, no use? Oft wird Vertrauen als kritischer Erfolgsfaktor propagiert, wenn es um die Nutzung von neuen Technologien geht, vor allem wenn es sich um intelligente Systeme, sogenannte KI (Künstliche Intelligenz) handelt. Diese finden nämlich immer mehr Eingang in die heutige Gesellschaft, sowohl im privaten (z. B. Einkaufen mit Amazons KI-basiertem Smart Speaker Alexa) als auch im beruflichen oder schulischen Umfeld (z. B. intelligente Systeme, die die Personalauswahl oder Lernprozesse unterstützen sollen). Der Einsatz von smarten, ubiquitären Technologien erhöht die Unsicherheit und Skepsis, gerade bei EndanwenderInnen ohne technisches Verständnis, und verschärft das Spannungsfeld zwischen Mensch, Maschine und Gesellschaft. Die Vertrauensfrage wird ins Rampenlicht gerückt. Ist Vertrauen der KonsumentInnen die Lösung, um das hochgelobte Potenzial der künstlichen Intelligenz voll auszunutzen? Ganz so einfach ist es nicht. Unklarheiten in Definitionen, Sprachgebrauch und Messmethoden verwässern das Verständnis um die Zusammenhänge von Vertrauen und Nutzen. Es ist über die unterschiedlichen Disziplinen hinweg nicht eindeutig geklärt, ob die Nutzung von neuen Technologien, insbesondere KI, tatsächlich mit Vertrauen einhergeht, für welchen Zweck und Vertrauen in wen: Die HerstellerInnen? Die DesignerInnen? Wie kann Vertrauen und Nutzung abgegrenzt werden? Dieser Beitrag hat zum Ziel, kuriose Geschichten und Behauptungen rund um KI und Vertrauen zu entmystifizieren, um letztlich Künstliche Intelligenz besser in die Praxis zu bringen. Dazu braucht es einen transdisziplinären und vor allem adressatengerechten Diskurs darüber, was intelligente Systeme sind, und eine differenzierte Auseinandersetzung damit, welche Rolle Vertrauen dabei spielen könnte. Es gibt nämlich auch die Vertrauens-SkeptikerInnen, die vehement die Meinung vertreten, dass Vertrauen im Kontext KI überhaupt keine Rolle spielt. Wir argumentieren, dass Vertrauen – vor allem im Endanwenderkontext – eine wichtige Variable ist, welche nicht nur die Adoption, sondern auch die Art und Weise, wie KI-basierte Systeme genutzt werden, maßgeblich beeinflusst. Anhand von praktischen Beispielen wollen wir aufzeigen, dass ein angemessen kalibriertes Vertrauensniveau nicht nur zu einem effizienteren, sicheren und synergetischen Umgang mit KI-basierten oder automatisierten Systemen führt, sondern sogar Leben retten kann.},
	language = {de},
	urldate = {2025-09-23},
	booktitle = {Kreativität und {Innovation} in {Organisationen} : {Impulse} aus {Innovationsforschung}, {Management}, {Kunst} und {Psychologie}},
	publisher = {Springer},
	author = {Tschopp, Marisa and Ruef, Marc and Monett, Dagmar},
	editor = {Landes, Miriam and Steiner, Eberhard and Utz, Tatjana},
	year = {2022},
	doi = {10.1007/978-3-662-63117-1_16},
	pages = {319--346},
}

@misc{schwartz_enhancing_2023,
	title = {Enhancing {Trust} in {LLM}-{Based} {AI} {Automation} {Agents}: {New} {Considerations} and {Future} {Challenges}},
	shorttitle = {Enhancing {Trust} in {LLM}-{Based} {AI} {Automation} {Agents}},
	url = {http://arxiv.org/abs/2308.05391},
	doi = {10.48550/arXiv.2308.05391},
	abstract = {Trust in AI agents has been extensively studied in the literature, resulting in significant advancements in our understanding of this field. However, the rapid advancements in Large Language Models (LLMs) and the emergence of LLM-based AI agent frameworks pose new challenges and opportunities for further research. In the field of process automation, a new generation of AI-based agents has emerged, enabling the execution of complex tasks. At the same time, the process of building automation has become more accessible to business users via user-friendly no-code tools and training mechanisms. This paper explores these new challenges and opportunities, analyzes the main aspects of trust in AI agents discussed in existing literature, and identifies specific considerations and challenges relevant to this new generation of automation agents. We also evaluate how nascent products in this category address these considerations. Finally, we highlight several challenges that the research community should address in this evolving landscape.},
	urldate = {2025-09-16},
	publisher = {arXiv},
	author = {Schwartz, Sivan and Yaeli, Avi and Shlomov, Segev},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05391 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{ding_citations_2025,
	title = {Citations and {Trust} in {LLM} {Generated} {Responses}},
	volume = {39},
	copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/34550},
	doi = {10.1609/aaai.v39i22.34550},
	abstract = {Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.},
	language = {en},
	number = {22},
	urldate = {2025-09-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ding, Yifan and Facciani, Matthew and Joyce, Ellen and Poudel, Amrit and Bhattacharya, Sanmitra and Veeramani, Balaji and Aguinaga, Sal and Weninger, Tim},
	month = apr,
	year = {2025},
	pages = {23787--23795},
}

@article{schlogl_chatbots_2024,
	series = {future internet 2024},
	title = {Chatbots in {Airport} {Customer} {Service}—{Exploring} {Use} {Cases} and {Technology} {Acceptance}},
	volume = {16},
	copyright = {CC BY 4.0},
	url = {https://www.researchgate.net/publication/380687481_Chatbots_in_Airport_Customer_Service-Exploring_Use_Cases_and_Technology_Acceptance},
	doi = {https://doi.org/10.3390/fi16050175},
	abstract = {Throughout the last decade, chatbots have gained widespread adoption across various industries, including healthcare, education, business, e-commerce, and entertainment. These types of artificial, usually cloud-based, agents have also been used in airport customer service, although there has been limited research concerning travelers’ perspectives on this rather techno-centric approach to handling inquiries. Consequently, the goal of the presented study was to tackle this research gap and explore potential use cases for chatbots at airports, as well as investigate travelers’ acceptance of said technology. We employed an extended version of the Technology Acceptance Model considering Perceived Usefulness, Perceived Ease of Use, Trust, and Perceived Enjoyment as predictors of Behavioral Intention, with Affinity for Technology as a potential moderator. A total of n=191 travelers completed our survey. The results show that Perceived Usefulness, Trust, Perceived Ease of Use, and Perceived Enjoyment positively correlate with the Behavioral Intention to use a chatbot for airport customer service inquiries, with Perceived Usefulness showing the highest impact. Travelers’ Affinity for Technology, on the other hand, does not seem to have any significant effect.},
	language = {en},
	number = {5},
	urldate = {2025-09-16},
	journal = {MDPI - Publisher of Open Access Journals},
	author = {Schlögl, Stephan and Auer, Isabelle and Gundula, Glowka},
	month = may,
	year = {2024},
	pages = {19},
}

@article{clegg_artificial_2024,
	title = {Artificial intelligence and management education: {A} conceptualization of human-machine interaction},
	volume = {22},
	issn = {1472-8117},
	shorttitle = {Artificial intelligence and management education},
	url = {https://www.sciencedirect.com/science/article/pii/S1472811724000788},
	doi = {10.1016/j.ijme.2024.101007},
	abstract = {The increasing use of Advanced Natural Language Processing (ANLP) models, particularly ChatGPT-4, presents opportunities and challenges to management education and research. These models can enhance the style, creativity, and analytical power of research papers, potentially shifting human scholars' roles from creators to ‘prompters’. If machines can perform educational and research tasks more effectively the role of human educators becomes a salient question in a world in which ANLP models offer clear, coherent, and polished insights, the use of which has potentially paradoxical possibilities. From one perspective, a new type of high-quality scholarship and education characterized by strong human involvement that synergistically leverages ANLP models' analytical capabilities, enabling human scholars to probe complex phenomena and make management research truly meaningful and impactful for broader audiences, is possible. We explore these questions through an ‘ideal type’ conceptualization of the possible relations between AI and management education and research.},
	number = {3},
	urldate = {2025-06-06},
	journal = {The International Journal of Management Education},
	author = {Clegg, Stewart and Sarkar, Soumodip},
	month = nov,
	year = {2024},
	keywords = {AI, Archetypes, ChatGPT, Management education},
	pages = {101007},
}

@article{li_developing_2024,
	title = {Developing trustworthy artificial intelligence: insights from research on interpersonal, human-automation, and human-{AI} trust},
	volume = {15},
	issn = {1664-1078},
	shorttitle = {Developing trustworthy artificial intelligence},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1382693/full},
	doi = {10.3389/fpsyg.2024.1382693},
	abstract = {The rapid advancement of artificial intelligence (AI) has impacted society in many aspects.Alongside this progress, concerns such as privacy violation, discriminatory bias, and safety risks have also surfaced, highlighting the need for the development of ethical, responsible, and socially beneficial AI. In response, the concept of trustworthy AI has gained prominence, and several guidelines for developing trustworthy AI have been proposed. Against this background, we demonstrate the significance of psychological research in identifying factors that contribute to the formation of trust in AI. Specifically, we review research findings on interpersonal, human-automation, and human-AI trust from the perspective of a threedimension framework (i.e., the trustor, the trustee, and their interactive context). The framework synthesizes common factors related to trust formation and maintenance across different trust types. These factors point out the foundational requirements for building trustworthy AI and provide pivotal guidance for its development that also involves communication, education, and training for users. We conclude by discussing how the insights in trust research can help enhance AI's trustworthiness and foster its adoption and application.},
	language = {English},
	urldate = {2025-06-03},
	journal = {Frontiers in Psychology},
	author = {Li, Yugang and Wu, Baizhou and Huang, Yuqi and Luan, Shenghua},
	month = apr,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {AI ethics, Human-Automation trust, Trustworthy AI, human-AI trust, interpersonal trust},
}

@article{riley_emotional_2024,
	title = {Emotional and cognitive trust in artificial intelligence: {A} framework for identifying research opportunities},
	volume = {58},
	issn = {2352-250X},
	shorttitle = {Emotional and cognitive trust in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S2352250X24000460},
	doi = {10.1016/j.copsyc.2024.101833},
	abstract = {This article briefly summarizes trust as a multi-dimensional construct, and trust in AI as a unique but related construct. It argues that because trust in AI is couched within an economic landscape, these two frameworks should be combined to understand the dynamics of trust in AI as it is currently implemented. The review focuses on healthcare and law enforcement as two industries that have adopted AI in ways that do and do not engender trust from stakeholders. The framework is applied to both industries to highlight where and why varying trust in AI is observed. Then seven research questions are posed, and researchers are encouraged to test the proposed framework in other AI-reliant contexts, like education and employment.},
	urldate = {2025-06-03},
	journal = {Current Opinion in Psychology},
	author = {Riley, Breagin K. and Dixon, Andrea},
	month = aug,
	year = {2024},
	keywords = {Artificial intelligence (AI), Healthcare, Law enforcement, Trust},
	pages = {101833},
}

@article{kuper_psychological_2025,
	title = {Psychological {Traits} and {Appropriate} {Reliance}: {Factors} {Shaping} {Trust} in {AI}},
	volume = {41},
	issn = {1044-7318},
	shorttitle = {Psychological {Traits} and {Appropriate} {Reliance}},
	url = {https://doi.org/10.1080/10447318.2024.2348216},
	doi = {10.1080/10447318.2024.2348216},
	abstract = {Research in AI-enabled decision support often focuses on technological factors influencing reliance on AI. However, the end-users of AI systems are individuals with diverse personalities which potentially lead to differences in collaborative human-computer interaction, resulting in harmful under- and over-reliance. The influence of psychological traits on appropriate reliance must be understood to enable development of more effective AI support addressing a diverse user base. This experimental mixed-methods study (N = 250) examined the impact of psychological traits on trust in and reliance on AI advice in classification tasks. Propensity to trust, affinity for technology interaction, and control beliefs in interacting with technology were identified as predictors for trust, which affect reliance. Thus, consideration must be given to the expected propensity to trust and the level of technological expertise among user groups when designing systems that aim to promote suitable degrees of trust and appropriate reliance.},
	number = {7},
	urldate = {2025-06-03},
	journal = {International Journal of Human–Computer Interaction},
	author = {Küper, Alisa and and Krämer, Nicole},
	month = apr,
	year = {2025},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2024.2348216},
	keywords = {AI reliance, appropriate reliance, decision support system, personality, trust},
	pages = {4115--4131},
}

@article{lukyanenko_trust_2022,
	title = {Trust in artificial intelligence: {From} a {Foundational} {Trust} {Framework} to emerging research opportunities},
	volume = {32},
	issn = {1422-8890},
	shorttitle = {Trust in artificial intelligence},
	url = {https://doi.org/10.1007/s12525-022-00605-4},
	doi = {10.1007/s12525-022-00605-4},
	abstract = {With the rise of artificial intelligence (AI), the issue of trust in AI emerges as a paramount societal concern. Despite increased attention of researchers, the topic remains fragmented without a common conceptual and theoretical foundation. To facilitate systematic research on this topic, we develop a Foundational Trust Framework to provide a conceptual, theoretical, and methodological foundation for trust research in general. The framework positions trust in general and trust in AI specifically as a problem of interaction among systems and applies systems thinking and general systems theory to trust and trust in AI. The Foundational Trust Framework is then used to gain a deeper understanding of the nature of trust in AI. From doing so, a research agenda emerges that proposes significant questions to facilitate further advances in empirical, theoretical, and design research on trust in AI.},
	language = {en},
	number = {4},
	urldate = {2025-06-03},
	journal = {Electronic Markets},
	author = {Lukyanenko, Roman and Maass, Wolfgang and Storey, Veda C.},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Artificial intelligence (AI), C71, C72, C73, C80, Computational Intelligence, D11, Explainable AI, Foundational Trust Framework, Intelligence Infrastructure, J00, L63, L64, L86, Logic in AI, Philosophy of Artificial Intelligence, Symbolic AI, Systems, Transparency, Trust, Trust in AI},
	pages = {1993--2020},
}

@article{abbass_social_2019,
	title = {Social {Integration} of {Artificial} {Intelligence}: {Functions}, {Automation} {Allocation} {Logic} and {Human}-{Autonomy} {Trust}},
	volume = {11},
	issn = {1866-9964},
	shorttitle = {Social {Integration} of {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s12559-018-9619-0},
	doi = {10.1007/s12559-018-9619-0},
	abstract = {Artificial intelligence (AI) is finding more uses in the human society resulting in a need to scrutinise the relationship between humans and AI. Technology itself has advanced from the mere encoding of human knowledge into a machine to designing machines that “know how” to autonomously acquire the knowledge they need, learn from it and act independently in the environment. Fortunately, this need is not new; it has scientific grounds that could be traced back to the inception of computers. This paper uses a multi-disciplinary lens to explore how the natural cognitive intelligence in a human could interface with the artificial cognitive intelligence of a machine. The scientific journey over the last 50 years will be examined to understand the Human-AI relationship, and to present the nature of, and the role of trust in, this relationship. Risks and opportunities sitting at the human-AI interface will be studied to reveal some of the fundamental technical challenges for a trustworthy human-AI relationship. The critical assessment of the literature leads to the conclusion that any social integration of AI into the human social system would necessitate a form of a relationship on one level or another in society, meaning that humans will “always” actively participate in certain decision-making loops—either in-the-loop or on-the-loop—that will influence the operations of AI, regardless of how sophisticated it is.},
	language = {en},
	number = {2},
	urldate = {2025-06-03},
	journal = {Cognitive Computation},
	author = {Abbass, Hussein A.},
	month = apr,
	year = {2019},
	keywords = {Adaptive aiding, Adaptive automation, Artificial Intelligence, Artificial intelligence, Augmented cognition, Automation logic, Cognitive cyber symbiosis, Function allocation, Human-AI teaming, Human-Machine Interfaces, Human-autonomy teaming, Humanitiy and Technology, Philosophy of Artificial Intelligence, Social Robotics, Symbolic AI, Trust},
	pages = {159--171},
}

@article{lockey_review_2021,
	title = {A {Review} of {Trust} in {Artificial} {Intelligence}: {Challenges}, {Vulnerabilities} and {Future} {Directions}},
	shorttitle = {A {Review} of {Trust} in {Artificial} {Intelligence}},
	url = {https://aisel.aisnet.org/hicss-54/os/trust/2},
	journal = {Hawaii International Conference on System Sciences 2021 (HICSS-54)},
	author = {Lockey, Steven and Gillespie, Nicole and Holm, Daniel and Someh, Ida Asadi},
	month = jan,
	year = {2021},
}

@article{sharan_effects_2020,
	title = {The effects of personality and locus of control on trust in humans versus artificial intelligence},
	volume = {6},
	issn = {2405-8440},
	url = {https://www.cell.com/heliyon/abstract/S2405-8440(20)31416-X},
	doi = {10.1016/j.heliyon.2020.e04572},
	language = {English},
	number = {8},
	urldate = {2025-06-03},
	journal = {Heliyon},
	author = {Sharan, Navya Nishith and Romano, Daniela Maria},
	month = aug,
	year = {2020},
	pmid = {32923706},
	note = {Publisher: Elsevier},
	keywords = {Artificial intelligence, Big five personality traits, Individual traits, Locus of control, Psychology, Trust},
}

@article{oksanen_trust_2020,
	title = {Trust {Toward} {Robots} and {Artificial} {Intelligence}: {An} {Experimental} {Approach} to {Human}–{Technology} {Interactions} {Online}},
	volume = {11},
	issn = {1664-1078},
	shorttitle = {Trust {Toward} {Robots} and {Artificial} {Intelligence}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.568256/full},
	doi = {10.3389/fpsyg.2020.568256},
	abstract = {Robotization and artificial intelligence (AI) are expected to change societies profoundly. Trust is an important factor of human–technology interactions, as robots and AI increasingly contribute to tasks previously handled by humans. Currently, there is a need for studies investigating trust toward AI and robots, especially in first-encounter meetings. This article reports findings from a study investigating trust toward robots and AI in an online trust game experiment. The trust game manipulated the hypothetical opponents that were described as either AI or robots. These were compared with control group opponents using only human name or nickname. Participants (N = 1077) lived in the United States. Describing opponents with robots or AI did not impact participants’ trust toward them. The robot called jdrx894 was the most trusted opponent. Opponents named “jdrx894” were trusted more than opponents called “Michael.” Further analysis showed that having a degree in technology or engineering, exposure to robots online and robot use self-efficacy predicted higher trust toward robots and AI. Out of Big Five personality characteristics, openness to experience predicted higher trust, and conscientiousness predicted lower trust. Results suggest trust on robots and AI is contextual and it also dependent on individual differences and knowledge on technology.},
	language = {English},
	urldate = {2025-06-03},
	journal = {Frontiers in Psychology},
	author = {Oksanen, Atte and Savela, Nina and Latikka, Rita and Koivula, Aki},
	month = dec,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Trust, artificial intelligence, human-technology interaction, individual differences, robot},
}

@article{dolgopolova_effect_2022,
	title = {The effect of attribute framing on consumers’ attitudes and intentions toward food: {A} {Meta}-analysis},
	volume = {10},
	shorttitle = {The effect of attribute framing on consumers’ attitudes and intentions toward food},
	doi = {10.36253/bae-11511},
	abstract = {This paper analyzes the existing literature on the effect of attribute framing on consumers’ attitudes and intentions with regard to food products. Attribute framing includes a broader interpretation of gains and losses when a product attribute is presented in a dichotomous way, such as fat vs. lean or harm vs. benefit. Meta-analysis results for the whole sample indicate that product attributes framed as gains have a higher effect on attitudes and intentions than product attributes framed as losses. Grouping studies by outcome variables, the meta-analysis demonstrates a larger effect size for studies that assess consumer attitude while for studies dealing with consumer intention, the effect size is close to zero and insignificant. We observe from the meta-regression results that the gain frame, the use of interaction terms, a specific product, and a student sample significantly influence consumers’ attitudes and intentions.},
	journal = {Bio-based and Applied Economics},
	author = {Dolgopolova, Irina and Li, Bingqing and Pirhonen, Helena and Roosen, Jutta},
	month = mar,
	year = {2022},
	pages = {253--264},
}


@online{alva_2025,
  author = {{Kanton Basel-Stadt}},
  title = {Alva - Der digitale Assistent},
  year = {2025},
  url = {https://www.bs.ch},
  urldate = {2025-12-03}
}

@online{kanton_basel_stadt_2025,
  author = {{Kanton Basel-Stadt}},
  title = {Offizielle Website des Kantons Basel-Stadt},
  year = {2025},
  url = {https://www.bs.ch},
  urldate = {2025-12-03}
}

@online{liip_2025,
  author = {{Liip AG}},
  title = {Liip - Digital Progress},
  year = {2025},
  url = {https://www.liip.ch},
  urldate = {2025-12-03}
}
